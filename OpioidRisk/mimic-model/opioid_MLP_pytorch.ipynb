{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary libraries for training model\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import *\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# load the data\n",
    "data_dir = '../mimic_data_after_preprocess/training_set_smote25.csv'\n",
    "train = pd.read_csv(data_dir)\n",
    "test = pd.read_csv('../mimic_data_after_preprocess/testing_set25.csv')\n",
    "simulation = pd.read_csv('../mimic_data_after_preprocess/simulation_set25.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23098 531\n",
      "In simulation set, male  12374  female  10724\n",
      "positive, male  234\n",
      "positive, female  297\n"
     ]
    }
   ],
   "source": [
    "print(len(simulation), sum(simulation['adverse_flag']==1))\n",
    "print(\"In simulation set, male \", sum(simulation['gender']==0), \" female \", sum(simulation['gender']==1))\n",
    "print(\"positive, male \", sum(simulation[simulation['gender']==0]['adverse_flag']==1))\n",
    "print(\"positive, female \", sum(simulation[simulation['gender']==1]['adverse_flag']==1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# remove unnecessary columns\n",
    "train = train.drop(['Unnamed: 0', 'age_1'], axis=1)\n",
    "test = test.drop(['subject_id', 'age_1'], axis=1)\n",
    "train_features = train.drop(['adverse_flag'], axis=1)\n",
    "train_labels = train['adverse_flag']\n",
    "test_features = test.drop(['adverse_flag'], axis=1)\n",
    "test_labels = test['adverse_flag']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# split test set into validation set and test set\n",
    "val_features, test_features, val_labels, test_labels = train_test_split(test_features,\n",
    "                                                                        test_labels,\n",
    "                                                                        test_size=0.5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features shape:  (90256, 24)\n",
      "train_labels shape:  (90256,)\n",
      "val_features shape:  (9239, 24)\n",
      "val_labels shape:  (9239,)\n",
      "test_features shape:  (9239, 24)\n",
      "test_labels shape:  (9239,)\n"
     ]
    }
   ],
   "source": [
    "# print the shape of the data\n",
    "print('train_features shape: ', train_features.shape)\n",
    "print('train_labels shape: ', train_labels.shape)\n",
    "print('val_features shape: ', val_features.shape)\n",
    "print('val_labels shape: ', val_labels.shape)\n",
    "print('test_features shape: ', test_features.shape)\n",
    "print('test_labels shape: ', test_labels.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# create a pytorch multi-layer perceptron model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_layers, hidden_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_size, hidden_size))\n",
    "        for i in range(hidden_layers - 1):\n",
    "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "        self.layers.append(nn.Linear(hidden_size, output_size))\n",
    "\n",
    "        self.activations = nn.ModuleList()\n",
    "        for i in range(hidden_layers):\n",
    "            self.activations.append(nn.ReLU())\n",
    "        self.activations.append(nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for i in range(self.hidden_layers + 1):\n",
    "            out = self.layers[i](out)\n",
    "            # if i != self.hidden_layers:\n",
    "            out = self.activations[i](out)\n",
    "        return out\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# define a function to train and evaluate a given model on a given dataset\n",
    "def train_and_eval(model, train_loader, test_loader, optimizer, criterion, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X.size(0)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in test_loader:\n",
    "                output = model(X)\n",
    "                loss = criterion(output, y)\n",
    "                test_loss += loss.item() * X.size(0)\n",
    "                predicted = output > 0.5\n",
    "                correct += (predicted == y).sum().item()\n",
    "                total += y.size(0)\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        test_acc = correct / total\n",
    "\n",
    "        print('Epoch [{}/{}], Train Loss: {:.4f}, Test Loss: {:.4f}, Test Acc: {:.2f}%'\n",
    "              .format(epoch+1, epochs, train_loss, test_loss, test_acc*100))\n",
    "\n",
    "    return test_acc\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# completed code that loops over different hyperparameter settings\n",
    "# define hyperparameters to search over\n",
    "param_grid = {\n",
    "    'hidden_layers': [5, 8, 10],\n",
    "    'hidden_size': [64, 128, 256],\n",
    "    'optimizer': [optim.Adam, optim.SGD],\n",
    "    'lr': [0.001, 0.01, 0.1]\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# check if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# turn pandas val_features, test_features, val_labels, test_labels into tensor\n",
    "X_train = torch.tensor(train_features.values, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(train_labels.values.reshape(-1, 1), dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(val_features.values, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(val_labels.values.reshape(-1, 1), dtype=torch.float32).to(device)\n",
    "# create train and test datasets and loaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([90256, 24])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start {'hidden_layers': 5, 'hidden_size': 64, 'lr': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Epoch [1/16], Train Loss: 0.3138, Test Loss: 0.2116, Test Acc: 92.67%\n",
      "Epoch [2/16], Train Loss: 0.2755, Test Loss: 0.1774, Test Acc: 95.01%\n",
      "Epoch [3/16], Train Loss: 0.2673, Test Loss: 0.1741, Test Acc: 94.37%\n",
      "Epoch [4/16], Train Loss: 0.2618, Test Loss: 0.1923, Test Acc: 93.22%\n",
      "Epoch [5/16], Train Loss: 0.2566, Test Loss: 0.1759, Test Acc: 93.39%\n",
      "Epoch [6/16], Train Loss: 0.2515, Test Loss: 0.1760, Test Acc: 94.45%\n",
      "Epoch [7/16], Train Loss: 0.2464, Test Loss: 0.1785, Test Acc: 94.14%\n",
      "Epoch [8/16], Train Loss: 0.2421, Test Loss: 0.1879, Test Acc: 92.81%\n",
      "Epoch [9/16], Train Loss: 0.2380, Test Loss: 0.1858, Test Acc: 93.32%\n",
      "Epoch [10/16], Train Loss: 0.2340, Test Loss: 0.1929, Test Acc: 93.02%\n",
      "Epoch [11/16], Train Loss: 0.2287, Test Loss: 0.1739, Test Acc: 93.77%\n",
      "Epoch [12/16], Train Loss: 0.2258, Test Loss: 0.1757, Test Acc: 93.68%\n",
      "Epoch [13/16], Train Loss: 0.2225, Test Loss: 0.1868, Test Acc: 92.69%\n",
      "Epoch [14/16], Train Loss: 0.2196, Test Loss: 0.1771, Test Acc: 94.06%\n",
      "Epoch [15/16], Train Loss: 0.2164, Test Loss: 0.1728, Test Acc: 94.36%\n",
      "Epoch [16/16], Train Loss: 0.2139, Test Loss: 0.1832, Test Acc: 94.18%\n",
      "End: 0.9417685896742072\n",
      "Current best: 0.9417685896742072\n",
      "Start {'hidden_layers': 5, 'hidden_size': 64, 'lr': 0.001, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Epoch [1/16], Train Loss: 0.5658, Test Loss: 0.3779, Test Acc: 97.39%\n",
      "Epoch [2/16], Train Loss: 0.4805, Test Loss: 0.2930, Test Acc: 96.90%\n",
      "Epoch [3/16], Train Loss: 0.4512, Test Loss: 0.2792, Test Acc: 96.24%\n",
      "Epoch [4/16], Train Loss: 0.4350, Test Loss: 0.2475, Test Acc: 96.76%\n",
      "Epoch [5/16], Train Loss: 0.4238, Test Loss: 0.2113, Test Acc: 97.34%\n",
      "Epoch [6/16], Train Loss: 0.4182, Test Loss: 0.2349, Test Acc: 96.49%\n",
      "Epoch [7/16], Train Loss: 0.4132, Test Loss: 0.2205, Test Acc: 96.69%\n",
      "Epoch [8/16], Train Loss: 0.4091, Test Loss: 0.2079, Test Acc: 96.89%\n",
      "Epoch [9/16], Train Loss: 0.4014, Test Loss: 0.2125, Test Acc: 96.70%\n",
      "Epoch [10/16], Train Loss: 0.3925, Test Loss: 0.2250, Test Acc: 96.48%\n",
      "Epoch [11/16], Train Loss: 0.3834, Test Loss: 0.2321, Test Acc: 96.32%\n",
      "Epoch [12/16], Train Loss: 0.3734, Test Loss: 0.2047, Test Acc: 96.88%\n",
      "Epoch [13/16], Train Loss: 0.3628, Test Loss: 0.2143, Test Acc: 96.50%\n",
      "Epoch [14/16], Train Loss: 0.3514, Test Loss: 0.2181, Test Acc: 96.21%\n",
      "Epoch [15/16], Train Loss: 0.3375, Test Loss: 0.2175, Test Acc: 95.85%\n",
      "Epoch [16/16], Train Loss: 0.3299, Test Loss: 0.1877, Test Acc: 96.51%\n",
      "End: 0.9651477432622578\n",
      "Current best: 0.9651477432622578\n",
      "Start {'hidden_layers': 5, 'hidden_size': 64, 'lr': 0.01, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Epoch [1/16], Train Loss: 0.3234, Test Loss: 0.1357, Test Acc: 95.93%\n",
      "Epoch [2/16], Train Loss: 0.2974, Test Loss: 0.1482, Test Acc: 96.61%\n",
      "Epoch [3/16], Train Loss: 0.2837, Test Loss: 0.2043, Test Acc: 93.14%\n",
      "Epoch [4/16], Train Loss: 0.2829, Test Loss: 0.1508, Test Acc: 95.65%\n",
      "Epoch [5/16], Train Loss: 0.2842, Test Loss: 0.1519, Test Acc: 96.34%\n",
      "Epoch [6/16], Train Loss: 0.2791, Test Loss: 0.1926, Test Acc: 94.93%\n",
      "Epoch [7/16], Train Loss: 0.2872, Test Loss: 0.1674, Test Acc: 95.85%\n",
      "Epoch [8/16], Train Loss: 0.2760, Test Loss: 0.1913, Test Acc: 94.67%\n",
      "Epoch [9/16], Train Loss: 0.2776, Test Loss: 0.1842, Test Acc: 95.48%\n",
      "Epoch [10/16], Train Loss: 0.3244, Test Loss: 0.2042, Test Acc: 93.32%\n",
      "Epoch [11/16], Train Loss: 0.2729, Test Loss: 0.1692, Test Acc: 95.83%\n",
      "Epoch [12/16], Train Loss: 0.2744, Test Loss: 0.1904, Test Acc: 96.40%\n",
      "Epoch [13/16], Train Loss: 0.3479, Test Loss: 0.2080, Test Acc: 93.21%\n",
      "Epoch [14/16], Train Loss: 0.3049, Test Loss: 0.1508, Test Acc: 95.79%\n",
      "Epoch [15/16], Train Loss: 0.2840, Test Loss: 0.1676, Test Acc: 92.91%\n",
      "Epoch [16/16], Train Loss: 0.2857, Test Loss: 0.1716, Test Acc: 95.44%\n",
      "End: 0.9544322978677346\n",
      "Current best: 0.9651477432622578\n",
      "Start {'hidden_layers': 5, 'hidden_size': 64, 'lr': 0.01, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Epoch [1/16], Train Loss: 0.4498, Test Loss: 0.2357, Test Acc: 96.69%\n",
      "Epoch [2/16], Train Loss: 0.3780, Test Loss: 0.2020, Test Acc: 96.51%\n",
      "Epoch [3/16], Train Loss: 0.3307, Test Loss: 0.2119, Test Acc: 95.52%\n",
      "Epoch [4/16], Train Loss: 0.3103, Test Loss: 0.1722, Test Acc: 96.59%\n",
      "Epoch [5/16], Train Loss: 0.3017, Test Loss: 0.3616, Test Acc: 85.80%\n",
      "Epoch [6/16], Train Loss: 0.2942, Test Loss: 0.1473, Test Acc: 97.09%\n",
      "Epoch [7/16], Train Loss: 0.2905, Test Loss: 0.1610, Test Acc: 95.72%\n",
      "Epoch [8/16], Train Loss: 0.2864, Test Loss: 0.1923, Test Acc: 94.52%\n",
      "Epoch [9/16], Train Loss: 0.2834, Test Loss: 0.1715, Test Acc: 95.87%\n",
      "Epoch [10/16], Train Loss: 0.2815, Test Loss: 0.2268, Test Acc: 91.78%\n",
      "Epoch [11/16], Train Loss: 0.2788, Test Loss: 0.1599, Test Acc: 95.91%\n",
      "Epoch [12/16], Train Loss: 0.2774, Test Loss: 0.2007, Test Acc: 94.50%\n",
      "Epoch [13/16], Train Loss: 0.2757, Test Loss: 0.2410, Test Acc: 92.06%\n",
      "Epoch [14/16], Train Loss: 0.2739, Test Loss: 0.1713, Test Acc: 95.71%\n",
      "Epoch [15/16], Train Loss: 0.2724, Test Loss: 0.2122, Test Acc: 92.90%\n",
      "Epoch [16/16], Train Loss: 0.2707, Test Loss: 0.1378, Test Acc: 96.46%\n",
      "End: 0.9646065591514233\n",
      "Current best: 0.9651477432622578\n",
      "Start {'hidden_layers': 5, 'hidden_size': 64, 'lr': 0.1, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Epoch [1/16], Train Loss: 0.6826, Test Loss: 0.2608, Test Acc: 97.63%\n",
      "Epoch [2/16], Train Loss: 0.5018, Test Loss: 0.2634, Test Acc: 97.63%\n",
      "Epoch [3/16], Train Loss: 0.5020, Test Loss: 0.2417, Test Acc: 97.63%\n",
      "Epoch [4/16], Train Loss: 0.5021, Test Loss: 0.3081, Test Acc: 97.63%\n",
      "Epoch [5/16], Train Loss: 0.5020, Test Loss: 0.2806, Test Acc: 97.63%\n",
      "Epoch [6/16], Train Loss: 0.5022, Test Loss: 0.2556, Test Acc: 97.63%\n",
      "Epoch [7/16], Train Loss: 0.5025, Test Loss: 0.2605, Test Acc: 97.63%\n",
      "Epoch [8/16], Train Loss: 0.5021, Test Loss: 0.2321, Test Acc: 97.63%\n",
      "Epoch [9/16], Train Loss: 0.5021, Test Loss: 0.2704, Test Acc: 97.63%\n",
      "Epoch [10/16], Train Loss: 0.5020, Test Loss: 0.2310, Test Acc: 97.63%\n",
      "Epoch [11/16], Train Loss: 0.5022, Test Loss: 0.2452, Test Acc: 97.63%\n",
      "Epoch [12/16], Train Loss: 0.5022, Test Loss: 0.2564, Test Acc: 97.63%\n",
      "Epoch [13/16], Train Loss: 0.5020, Test Loss: 0.2694, Test Acc: 97.63%\n",
      "Epoch [14/16], Train Loss: 0.5021, Test Loss: 0.2597, Test Acc: 97.63%\n",
      "Epoch [15/16], Train Loss: 0.5017, Test Loss: 0.2558, Test Acc: 97.63%\n",
      "Epoch [16/16], Train Loss: 0.5021, Test Loss: 0.2417, Test Acc: 97.63%\n",
      "End: 0.9762961359454486\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 5, 'hidden_size': 64, 'lr': 0.1, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Epoch [1/16], Train Loss: 0.3629, Test Loss: 0.1952, Test Acc: 94.21%\n",
      "Epoch [2/16], Train Loss: 0.3191, Test Loss: 0.2046, Test Acc: 94.31%\n",
      "Epoch [3/16], Train Loss: 0.3055, Test Loss: 0.1677, Test Acc: 95.12%\n",
      "Epoch [4/16], Train Loss: 0.2974, Test Loss: 0.1464, Test Acc: 96.07%\n",
      "Epoch [5/16], Train Loss: 0.2923, Test Loss: 0.2757, Test Acc: 88.82%\n",
      "Epoch [6/16], Train Loss: 0.2891, Test Loss: 0.2919, Test Acc: 87.61%\n",
      "Epoch [7/16], Train Loss: 0.2882, Test Loss: 0.1523, Test Acc: 95.91%\n",
      "Epoch [8/16], Train Loss: 0.2836, Test Loss: 0.1366, Test Acc: 96.86%\n",
      "Epoch [9/16], Train Loss: 0.2806, Test Loss: 0.1806, Test Acc: 94.53%\n",
      "Epoch [10/16], Train Loss: 0.2768, Test Loss: 0.1631, Test Acc: 95.11%\n",
      "Epoch [11/16], Train Loss: 0.2756, Test Loss: 0.1847, Test Acc: 94.19%\n",
      "Epoch [12/16], Train Loss: 0.2724, Test Loss: 0.1630, Test Acc: 95.33%\n",
      "Epoch [13/16], Train Loss: 0.2696, Test Loss: 0.2294, Test Acc: 89.92%\n",
      "Epoch [14/16], Train Loss: 0.2679, Test Loss: 0.1919, Test Acc: 93.65%\n",
      "Epoch [15/16], Train Loss: 0.2668, Test Loss: 0.1686, Test Acc: 94.48%\n",
      "Epoch [16/16], Train Loss: 0.2642, Test Loss: 0.1501, Test Acc: 95.72%\n",
      "End: 0.957246455244074\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 5, 'hidden_size': 128, 'lr': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Epoch [1/16], Train Loss: 0.3075, Test Loss: 0.1641, Test Acc: 95.54%\n",
      "Epoch [2/16], Train Loss: 0.2743, Test Loss: 0.1771, Test Acc: 94.57%\n",
      "Epoch [3/16], Train Loss: 0.2657, Test Loss: 0.1653, Test Acc: 95.18%\n",
      "Epoch [4/16], Train Loss: 0.2583, Test Loss: 0.1517, Test Acc: 95.49%\n",
      "Epoch [5/16], Train Loss: 0.2526, Test Loss: 0.1585, Test Acc: 94.59%\n",
      "Epoch [6/16], Train Loss: 0.2457, Test Loss: 0.1760, Test Acc: 94.59%\n",
      "Epoch [7/16], Train Loss: 0.2409, Test Loss: 0.2115, Test Acc: 94.33%\n",
      "Epoch [8/16], Train Loss: 0.2358, Test Loss: 0.2007, Test Acc: 93.75%\n",
      "Epoch [9/16], Train Loss: 0.2306, Test Loss: 0.1818, Test Acc: 94.70%\n",
      "Epoch [10/16], Train Loss: 0.2261, Test Loss: 0.2008, Test Acc: 93.72%\n",
      "Epoch [11/16], Train Loss: 0.2231, Test Loss: 0.2207, Test Acc: 92.94%\n",
      "Epoch [12/16], Train Loss: 0.2189, Test Loss: 0.3275, Test Acc: 94.07%\n",
      "Epoch [13/16], Train Loss: 0.2158, Test Loss: 0.2024, Test Acc: 93.48%\n",
      "Epoch [14/16], Train Loss: 0.2131, Test Loss: 0.2082, Test Acc: 93.84%\n",
      "Epoch [15/16], Train Loss: 0.2093, Test Loss: 0.2344, Test Acc: 93.87%\n",
      "Epoch [16/16], Train Loss: 0.2064, Test Loss: 0.2285, Test Acc: 93.00%\n",
      "End: 0.9299707760580149\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 5, 'hidden_size': 128, 'lr': 0.001, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Epoch [1/16], Train Loss: 0.5646, Test Loss: 0.3760, Test Acc: 97.34%\n",
      "Epoch [2/16], Train Loss: 0.4722, Test Loss: 0.2880, Test Acc: 97.11%\n",
      "Epoch [3/16], Train Loss: 0.4380, Test Loss: 0.2742, Test Acc: 96.10%\n",
      "Epoch [4/16], Train Loss: 0.4285, Test Loss: 0.2370, Test Acc: 96.74%\n",
      "Epoch [5/16], Train Loss: 0.4201, Test Loss: 0.2318, Test Acc: 96.57%\n",
      "Epoch [6/16], Train Loss: 0.4123, Test Loss: 0.2080, Test Acc: 96.85%\n",
      "Epoch [7/16], Train Loss: 0.4010, Test Loss: 0.2086, Test Acc: 96.71%\n",
      "Epoch [8/16], Train Loss: 0.3912, Test Loss: 0.2042, Test Acc: 96.81%\n",
      "Epoch [9/16], Train Loss: 0.3816, Test Loss: 0.1897, Test Acc: 96.84%\n",
      "Epoch [10/16], Train Loss: 0.3719, Test Loss: 0.2338, Test Acc: 95.45%\n",
      "Epoch [11/16], Train Loss: 0.3615, Test Loss: 0.2226, Test Acc: 95.78%\n",
      "Epoch [12/16], Train Loss: 0.3482, Test Loss: 0.1805, Test Acc: 96.82%\n",
      "Epoch [13/16], Train Loss: 0.3356, Test Loss: 0.2120, Test Acc: 96.09%\n",
      "Epoch [14/16], Train Loss: 0.3282, Test Loss: 0.1757, Test Acc: 96.16%\n",
      "Epoch [15/16], Train Loss: 0.3190, Test Loss: 0.2090, Test Acc: 95.19%\n",
      "Epoch [16/16], Train Loss: 0.3145, Test Loss: 0.1590, Test Acc: 96.30%\n",
      "End: 0.9629830068189198\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 5, 'hidden_size': 128, 'lr': 0.01, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Epoch [1/16], Train Loss: 0.3376, Test Loss: 0.1400, Test Acc: 95.55%\n",
      "Epoch [2/16], Train Loss: 0.3730, Test Loss: 0.1578, Test Acc: 94.73%\n",
      "Epoch [3/16], Train Loss: 0.3042, Test Loss: 0.1580, Test Acc: 95.80%\n",
      "Epoch [4/16], Train Loss: 0.5241, Test Loss: 0.3092, Test Acc: 97.63%\n",
      "Epoch [5/16], Train Loss: 0.5832, Test Loss: 0.2507, Test Acc: 97.63%\n",
      "Epoch [6/16], Train Loss: 0.7556, Test Loss: 0.2647, Test Acc: 97.63%\n",
      "Epoch [7/16], Train Loss: 0.6377, Test Loss: 0.2648, Test Acc: 97.63%\n",
      "Epoch [8/16], Train Loss: 0.5794, Test Loss: 0.3622, Test Acc: 97.63%\n",
      "Epoch [9/16], Train Loss: 0.6027, Test Loss: 0.2773, Test Acc: 97.63%\n",
      "Epoch [10/16], Train Loss: 0.5458, Test Loss: 0.2853, Test Acc: 97.63%\n",
      "Epoch [11/16], Train Loss: 0.5458, Test Loss: 0.2885, Test Acc: 97.63%\n",
      "Epoch [12/16], Train Loss: 0.5458, Test Loss: 0.2714, Test Acc: 97.63%\n",
      "Epoch [13/16], Train Loss: 0.5458, Test Loss: 0.2888, Test Acc: 97.63%\n",
      "Epoch [14/16], Train Loss: 0.5458, Test Loss: 0.2780, Test Acc: 97.63%\n",
      "Epoch [15/16], Train Loss: 0.5458, Test Loss: 0.2804, Test Acc: 97.63%\n",
      "Epoch [16/16], Train Loss: 0.5457, Test Loss: 0.2673, Test Acc: 97.63%\n",
      "End: 0.9762961359454486\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 5, 'hidden_size': 128, 'lr': 0.01, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Epoch [1/16], Train Loss: 0.4424, Test Loss: 0.2520, Test Acc: 95.81%\n",
      "Epoch [2/16], Train Loss: 0.3548, Test Loss: 0.2966, Test Acc: 93.48%\n",
      "Epoch [3/16], Train Loss: 0.3192, Test Loss: 0.3068, Test Acc: 87.44%\n",
      "Epoch [4/16], Train Loss: 0.3024, Test Loss: 0.1547, Test Acc: 96.54%\n",
      "Epoch [5/16], Train Loss: 0.2945, Test Loss: 0.2100, Test Acc: 94.35%\n",
      "Epoch [6/16], Train Loss: 0.2883, Test Loss: 0.1439, Test Acc: 96.67%\n",
      "Epoch [7/16], Train Loss: 0.2843, Test Loss: 0.1170, Test Acc: 97.03%\n",
      "Epoch [8/16], Train Loss: 0.2808, Test Loss: 0.2092, Test Acc: 92.76%\n",
      "Epoch [9/16], Train Loss: 0.2784, Test Loss: 0.2614, Test Acc: 88.38%\n",
      "Epoch [10/16], Train Loss: 0.2757, Test Loss: 0.1931, Test Acc: 92.91%\n",
      "Epoch [11/16], Train Loss: 0.2731, Test Loss: 0.1517, Test Acc: 95.53%\n",
      "Epoch [12/16], Train Loss: 0.2716, Test Loss: 0.2071, Test Acc: 92.49%\n",
      "Epoch [13/16], Train Loss: 0.2686, Test Loss: 0.1235, Test Acc: 96.63%\n",
      "Epoch [14/16], Train Loss: 0.2666, Test Loss: 0.1771, Test Acc: 93.95%\n",
      "Epoch [15/16], Train Loss: 0.2653, Test Loss: 0.1954, Test Acc: 93.29%\n",
      "Epoch [16/16], Train Loss: 0.2638, Test Loss: 0.1708, Test Acc: 94.45%\n",
      "End: 0.9444745102283797\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 5, 'hidden_size': 128, 'lr': 0.1, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Epoch [1/16], Train Loss: 19.9923, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [2/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [3/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [4/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [5/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [6/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [7/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [8/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [9/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [10/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [11/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [12/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [13/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [14/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [15/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [16/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "End: 0.9762961359454486\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 5, 'hidden_size': 128, 'lr': 0.1, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Epoch [1/16], Train Loss: 0.3616, Test Loss: 0.1639, Test Acc: 95.41%\n",
      "Epoch [2/16], Train Loss: 0.3116, Test Loss: 0.1706, Test Acc: 96.04%\n",
      "Epoch [3/16], Train Loss: 0.2995, Test Loss: 0.1324, Test Acc: 96.87%\n",
      "Epoch [4/16], Train Loss: 0.2914, Test Loss: 0.4257, Test Acc: 79.20%\n",
      "Epoch [5/16], Train Loss: 0.2868, Test Loss: 0.2117, Test Acc: 93.28%\n",
      "Epoch [6/16], Train Loss: 0.2835, Test Loss: 0.1355, Test Acc: 96.67%\n",
      "Epoch [7/16], Train Loss: 0.2781, Test Loss: 0.1687, Test Acc: 95.06%\n",
      "Epoch [8/16], Train Loss: 0.2749, Test Loss: 0.1783, Test Acc: 95.57%\n",
      "Epoch [9/16], Train Loss: 0.2730, Test Loss: 0.1112, Test Acc: 97.21%\n",
      "Epoch [10/16], Train Loss: 0.2685, Test Loss: 0.1945, Test Acc: 92.93%\n",
      "Epoch [11/16], Train Loss: 0.2667, Test Loss: 0.1584, Test Acc: 95.93%\n",
      "Epoch [12/16], Train Loss: 0.2632, Test Loss: 0.1686, Test Acc: 94.72%\n",
      "Epoch [13/16], Train Loss: 0.2601, Test Loss: 0.1554, Test Acc: 94.80%\n",
      "Epoch [14/16], Train Loss: 0.2598, Test Loss: 0.2069, Test Acc: 92.00%\n",
      "Epoch [15/16], Train Loss: 0.2575, Test Loss: 0.1783, Test Acc: 95.32%\n",
      "Epoch [16/16], Train Loss: 0.2550, Test Loss: 0.2309, Test Acc: 91.25%\n",
      "End: 0.9125446476891439\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 5, 'hidden_size': 256, 'lr': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Epoch [1/16], Train Loss: 0.3136, Test Loss: 0.2226, Test Acc: 92.51%\n",
      "Epoch [2/16], Train Loss: 0.2770, Test Loss: 0.1624, Test Acc: 94.30%\n",
      "Epoch [3/16], Train Loss: 0.2668, Test Loss: 0.1640, Test Acc: 94.96%\n",
      "Epoch [4/16], Train Loss: 0.2587, Test Loss: 0.1721, Test Acc: 93.67%\n",
      "Epoch [5/16], Train Loss: 0.2517, Test Loss: 0.2008, Test Acc: 92.98%\n",
      "Epoch [6/16], Train Loss: 0.2447, Test Loss: 0.1807, Test Acc: 94.38%\n",
      "Epoch [7/16], Train Loss: 0.2401, Test Loss: 0.1787, Test Acc: 94.17%\n",
      "Epoch [8/16], Train Loss: 0.2351, Test Loss: 0.1987, Test Acc: 94.03%\n",
      "Epoch [9/16], Train Loss: 0.2307, Test Loss: 0.2165, Test Acc: 94.08%\n",
      "Epoch [10/16], Train Loss: 0.2271, Test Loss: 0.1816, Test Acc: 94.67%\n",
      "Epoch [11/16], Train Loss: 0.2221, Test Loss: 0.2890, Test Acc: 95.17%\n",
      "Epoch [12/16], Train Loss: 0.2194, Test Loss: 0.2815, Test Acc: 92.61%\n",
      "Epoch [13/16], Train Loss: 0.2148, Test Loss: 0.2314, Test Acc: 93.73%\n",
      "Epoch [14/16], Train Loss: 0.2119, Test Loss: 0.3285, Test Acc: 94.17%\n",
      "Epoch [15/16], Train Loss: 0.2094, Test Loss: 0.2267, Test Acc: 93.99%\n",
      "Epoch [16/16], Train Loss: 0.2065, Test Loss: 0.2884, Test Acc: 93.28%\n",
      "End: 0.9327849334343543\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 5, 'hidden_size': 256, 'lr': 0.001, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Epoch [1/16], Train Loss: 0.5824, Test Loss: 0.3871, Test Acc: 97.43%\n",
      "Epoch [2/16], Train Loss: 0.4708, Test Loss: 0.2887, Test Acc: 97.19%\n",
      "Epoch [3/16], Train Loss: 0.4379, Test Loss: 0.2421, Test Acc: 97.14%\n",
      "Epoch [4/16], Train Loss: 0.4262, Test Loss: 0.2467, Test Acc: 96.29%\n",
      "Epoch [5/16], Train Loss: 0.4144, Test Loss: 0.2085, Test Acc: 97.03%\n",
      "Epoch [6/16], Train Loss: 0.4017, Test Loss: 0.2184, Test Acc: 96.33%\n",
      "Epoch [7/16], Train Loss: 0.3915, Test Loss: 0.2076, Test Acc: 96.53%\n",
      "Epoch [8/16], Train Loss: 0.3784, Test Loss: 0.1882, Test Acc: 96.74%\n",
      "Epoch [9/16], Train Loss: 0.3654, Test Loss: 0.1972, Test Acc: 96.34%\n",
      "Epoch [10/16], Train Loss: 0.3526, Test Loss: 0.2393, Test Acc: 94.93%\n",
      "Epoch [11/16], Train Loss: 0.3408, Test Loss: 0.1596, Test Acc: 97.25%\n",
      "Epoch [12/16], Train Loss: 0.3306, Test Loss: 0.1488, Test Acc: 97.03%\n",
      "Epoch [13/16], Train Loss: 0.3221, Test Loss: 0.1912, Test Acc: 95.70%\n",
      "Epoch [14/16], Train Loss: 0.3175, Test Loss: 0.1650, Test Acc: 96.03%\n",
      "Epoch [15/16], Train Loss: 0.3101, Test Loss: 0.1432, Test Acc: 96.89%\n",
      "Epoch [16/16], Train Loss: 0.3066, Test Loss: 0.2026, Test Acc: 94.25%\n",
      "End: 0.9425262474293755\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 5, 'hidden_size': 256, 'lr': 0.01, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Epoch [1/16], Train Loss: 0.6166, Test Loss: 0.2642, Test Acc: 97.63%\n",
      "Epoch [2/16], Train Loss: 1.8942, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [3/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [4/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [5/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [6/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [7/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [8/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [9/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [10/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [11/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [12/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [13/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [14/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [15/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [16/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "End: 0.9762961359454486\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 5, 'hidden_size': 256, 'lr': 0.01, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Epoch [1/16], Train Loss: 0.4407, Test Loss: 0.2056, Test Acc: 97.09%\n",
      "Epoch [2/16], Train Loss: 0.3577, Test Loss: 0.3009, Test Acc: 92.51%\n",
      "Epoch [3/16], Train Loss: 0.3153, Test Loss: 0.1378, Test Acc: 96.84%\n",
      "Epoch [4/16], Train Loss: 0.2989, Test Loss: 0.2255, Test Acc: 93.41%\n",
      "Epoch [5/16], Train Loss: 0.2901, Test Loss: 0.2888, Test Acc: 88.62%\n",
      "Epoch [6/16], Train Loss: 0.2849, Test Loss: 0.2500, Test Acc: 90.68%\n",
      "Epoch [7/16], Train Loss: 0.2802, Test Loss: 0.1285, Test Acc: 96.80%\n",
      "Epoch [8/16], Train Loss: 0.2762, Test Loss: 0.2934, Test Acc: 86.13%\n",
      "Epoch [9/16], Train Loss: 0.2735, Test Loss: 0.1423, Test Acc: 96.16%\n",
      "Epoch [10/16], Train Loss: 0.2707, Test Loss: 0.2340, Test Acc: 90.00%\n",
      "Epoch [11/16], Train Loss: 0.2676, Test Loss: 0.1586, Test Acc: 95.62%\n",
      "Epoch [12/16], Train Loss: 0.2658, Test Loss: 0.1762, Test Acc: 93.58%\n",
      "Epoch [13/16], Train Loss: 0.2623, Test Loss: 0.1450, Test Acc: 95.76%\n",
      "Epoch [14/16], Train Loss: 0.2600, Test Loss: 0.1823, Test Acc: 93.82%\n",
      "Epoch [15/16], Train Loss: 0.2579, Test Loss: 0.1628, Test Acc: 93.85%\n",
      "Epoch [16/16], Train Loss: 0.2556, Test Loss: 0.1604, Test Acc: 94.45%\n",
      "End: 0.9444745102283797\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 5, 'hidden_size': 256, 'lr': 0.1, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Epoch [1/16], Train Loss: 19.9923, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [2/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [3/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [4/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [5/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [6/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [7/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [8/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [9/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [10/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [11/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [12/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [13/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [14/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [15/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [16/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "End: 0.9762961359454486\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 5, 'hidden_size': 256, 'lr': 0.1, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Epoch [1/16], Train Loss: 0.3645, Test Loss: 0.2001, Test Acc: 95.06%\n",
      "Epoch [2/16], Train Loss: 0.3093, Test Loss: 0.1607, Test Acc: 95.23%\n",
      "Epoch [3/16], Train Loss: 0.2974, Test Loss: 0.1490, Test Acc: 96.54%\n",
      "Epoch [4/16], Train Loss: 0.2893, Test Loss: 0.1750, Test Acc: 95.01%\n",
      "Epoch [5/16], Train Loss: 0.2830, Test Loss: 0.1897, Test Acc: 93.93%\n",
      "Epoch [6/16], Train Loss: 0.2782, Test Loss: 0.1981, Test Acc: 94.79%\n",
      "Epoch [7/16], Train Loss: 0.2733, Test Loss: 0.2342, Test Acc: 91.29%\n",
      "Epoch [8/16], Train Loss: 0.2701, Test Loss: 0.1748, Test Acc: 93.37%\n",
      "Epoch [9/16], Train Loss: 0.2661, Test Loss: 0.1415, Test Acc: 96.08%\n",
      "Epoch [10/16], Train Loss: 0.2629, Test Loss: 0.1565, Test Acc: 96.00%\n",
      "Epoch [11/16], Train Loss: 0.2592, Test Loss: 0.3108, Test Acc: 83.14%\n",
      "Epoch [12/16], Train Loss: 0.2568, Test Loss: 0.1979, Test Acc: 93.28%\n",
      "Epoch [13/16], Train Loss: 0.2535, Test Loss: 0.1679, Test Acc: 94.95%\n",
      "Epoch [14/16], Train Loss: 0.2505, Test Loss: 0.1914, Test Acc: 92.49%\n",
      "Epoch [15/16], Train Loss: 0.2471, Test Loss: 0.1867, Test Acc: 92.47%\n",
      "Epoch [16/16], Train Loss: 0.2448, Test Loss: 0.1274, Test Acc: 96.28%\n",
      "End: 0.962766533174586\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 8, 'hidden_size': 64, 'lr': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Epoch [1/16], Train Loss: 0.3095, Test Loss: 0.1643, Test Acc: 94.39%\n",
      "Epoch [2/16], Train Loss: 0.2771, Test Loss: 0.1690, Test Acc: 94.60%\n",
      "Epoch [3/16], Train Loss: 0.2689, Test Loss: 0.1849, Test Acc: 94.24%\n",
      "Epoch [4/16], Train Loss: 0.2641, Test Loss: 0.1491, Test Acc: 95.35%\n",
      "Epoch [5/16], Train Loss: 0.2595, Test Loss: 0.1660, Test Acc: 94.47%\n",
      "Epoch [6/16], Train Loss: 0.2538, Test Loss: 0.1658, Test Acc: 94.88%\n",
      "Epoch [7/16], Train Loss: 0.2505, Test Loss: 0.1617, Test Acc: 94.96%\n",
      "Epoch [8/16], Train Loss: 0.2467, Test Loss: 0.1845, Test Acc: 93.00%\n",
      "Epoch [9/16], Train Loss: 0.2422, Test Loss: 0.1591, Test Acc: 95.33%\n",
      "Epoch [10/16], Train Loss: 0.2402, Test Loss: 0.1855, Test Acc: 93.58%\n",
      "Epoch [11/16], Train Loss: 0.2365, Test Loss: 0.1792, Test Acc: 93.54%\n",
      "Epoch [12/16], Train Loss: 0.2337, Test Loss: 0.1749, Test Acc: 93.93%\n",
      "Epoch [13/16], Train Loss: 0.2319, Test Loss: 0.1757, Test Acc: 94.23%\n",
      "Epoch [14/16], Train Loss: 0.2285, Test Loss: 0.1808, Test Acc: 93.26%\n",
      "Epoch [15/16], Train Loss: 0.2269, Test Loss: 0.1695, Test Acc: 93.87%\n",
      "Epoch [16/16], Train Loss: 0.2246, Test Loss: 0.1746, Test Acc: 94.18%\n",
      "End: 0.9417685896742072\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 8, 'hidden_size': 64, 'lr': 0.001, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Epoch [1/16], Train Loss: 0.5878, Test Loss: 0.3821, Test Acc: 97.63%\n",
      "Epoch [2/16], Train Loss: 0.5116, Test Loss: 0.2939, Test Acc: 97.63%\n",
      "Epoch [3/16], Train Loss: 0.5000, Test Loss: 0.2671, Test Acc: 97.63%\n",
      "Epoch [4/16], Train Loss: 0.4959, Test Loss: 0.2571, Test Acc: 97.63%\n",
      "Epoch [5/16], Train Loss: 0.4900, Test Loss: 0.2538, Test Acc: 97.63%\n",
      "Epoch [6/16], Train Loss: 0.4769, Test Loss: 0.2540, Test Acc: 97.43%\n",
      "Epoch [7/16], Train Loss: 0.4588, Test Loss: 0.2510, Test Acc: 96.67%\n",
      "Epoch [8/16], Train Loss: 0.4503, Test Loss: 0.2422, Test Acc: 96.36%\n",
      "Epoch [9/16], Train Loss: 0.4443, Test Loss: 0.2316, Test Acc: 96.41%\n",
      "Epoch [10/16], Train Loss: 0.4348, Test Loss: 0.2330, Test Acc: 96.46%\n",
      "Epoch [11/16], Train Loss: 0.4247, Test Loss: 0.2333, Test Acc: 96.48%\n",
      "Epoch [12/16], Train Loss: 0.4137, Test Loss: 0.2241, Test Acc: 96.58%\n",
      "Epoch [13/16], Train Loss: 0.4077, Test Loss: 0.2225, Test Acc: 96.54%\n",
      "Epoch [14/16], Train Loss: 0.4008, Test Loss: 0.2172, Test Acc: 96.73%\n",
      "Epoch [15/16], Train Loss: 0.3919, Test Loss: 0.2221, Test Acc: 96.30%\n",
      "Epoch [16/16], Train Loss: 0.3786, Test Loss: 0.2343, Test Acc: 95.75%\n",
      "End: 0.9574629288884078\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 8, 'hidden_size': 64, 'lr': 0.01, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Epoch [1/16], Train Loss: 0.7041, Test Loss: 0.2627, Test Acc: 97.63%\n",
      "Epoch [2/16], Train Loss: 0.5009, Test Loss: 0.2724, Test Acc: 97.63%\n",
      "Epoch [3/16], Train Loss: 0.5823, Test Loss: 0.2457, Test Acc: 97.63%\n",
      "Epoch [4/16], Train Loss: 0.4999, Test Loss: 0.2510, Test Acc: 97.63%\n",
      "Epoch [5/16], Train Loss: 0.4999, Test Loss: 0.2514, Test Acc: 97.63%\n",
      "Epoch [6/16], Train Loss: 0.4997, Test Loss: 0.2431, Test Acc: 97.63%\n",
      "Epoch [7/16], Train Loss: 0.4998, Test Loss: 0.2557, Test Acc: 97.63%\n",
      "Epoch [8/16], Train Loss: 0.4997, Test Loss: 0.2657, Test Acc: 97.63%\n",
      "Epoch [9/16], Train Loss: 0.7364, Test Loss: 0.2495, Test Acc: 97.63%\n",
      "Epoch [10/16], Train Loss: 0.5006, Test Loss: 0.2550, Test Acc: 97.63%\n",
      "Epoch [11/16], Train Loss: 0.5006, Test Loss: 0.2533, Test Acc: 97.63%\n",
      "Epoch [12/16], Train Loss: 0.5006, Test Loss: 0.2522, Test Acc: 97.63%\n",
      "Epoch [13/16], Train Loss: 0.5006, Test Loss: 0.2503, Test Acc: 97.63%\n",
      "Epoch [14/16], Train Loss: 0.5006, Test Loss: 0.2673, Test Acc: 97.63%\n",
      "Epoch [15/16], Train Loss: 0.5005, Test Loss: 0.2784, Test Acc: 97.63%\n",
      "Epoch [16/16], Train Loss: 0.5006, Test Loss: 0.2632, Test Acc: 97.63%\n",
      "End: 0.9762961359454486\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 8, 'hidden_size': 64, 'lr': 0.01, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Epoch [1/16], Train Loss: 0.4703, Test Loss: 0.2297, Test Acc: 96.61%\n",
      "Epoch [2/16], Train Loss: 0.3851, Test Loss: 0.1589, Test Acc: 96.96%\n",
      "Epoch [3/16], Train Loss: 0.3364, Test Loss: 0.2425, Test Acc: 93.25%\n",
      "Epoch [4/16], Train Loss: 0.3127, Test Loss: 0.1875, Test Acc: 94.74%\n",
      "Epoch [5/16], Train Loss: 0.3003, Test Loss: 0.2221, Test Acc: 93.74%\n",
      "Epoch [6/16], Train Loss: 0.2935, Test Loss: 0.1436, Test Acc: 96.58%\n",
      "Epoch [7/16], Train Loss: 0.2888, Test Loss: 0.1563, Test Acc: 96.18%\n",
      "Epoch [8/16], Train Loss: 0.2845, Test Loss: 0.2126, Test Acc: 93.54%\n",
      "Epoch [9/16], Train Loss: 0.2817, Test Loss: 0.2435, Test Acc: 92.02%\n",
      "Epoch [10/16], Train Loss: 0.2794, Test Loss: 0.2688, Test Acc: 91.05%\n",
      "Epoch [11/16], Train Loss: 0.2764, Test Loss: 0.1672, Test Acc: 95.27%\n",
      "Epoch [12/16], Train Loss: 0.2750, Test Loss: 0.2006, Test Acc: 93.47%\n",
      "Epoch [13/16], Train Loss: 0.2725, Test Loss: 0.2110, Test Acc: 92.33%\n",
      "Epoch [14/16], Train Loss: 0.2701, Test Loss: 0.1652, Test Acc: 94.61%\n",
      "Epoch [15/16], Train Loss: 0.2695, Test Loss: 0.1704, Test Acc: 94.84%\n",
      "Epoch [16/16], Train Loss: 0.2672, Test Loss: 0.1993, Test Acc: 93.65%\n",
      "End: 0.936464985388029\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 8, 'hidden_size': 64, 'lr': 0.1, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Epoch [1/16], Train Loss: 35.6911, Test Loss: 30.2197, Test Acc: 69.78%\n",
      "Epoch [2/16], Train Loss: 35.0503, Test Loss: 30.2197, Test Acc: 69.78%\n",
      "Epoch [3/16], Train Loss: 35.0503, Test Loss: 30.2197, Test Acc: 69.78%\n",
      "Epoch [4/16], Train Loss: 35.0503, Test Loss: 30.2197, Test Acc: 69.78%\n",
      "Epoch [5/16], Train Loss: 35.0503, Test Loss: 30.2197, Test Acc: 69.78%\n",
      "Epoch [6/16], Train Loss: 35.0503, Test Loss: 30.2197, Test Acc: 69.78%\n",
      "Epoch [7/16], Train Loss: 35.0503, Test Loss: 30.2197, Test Acc: 69.78%\n",
      "Epoch [8/16], Train Loss: 35.0503, Test Loss: 30.2197, Test Acc: 69.78%\n",
      "Epoch [9/16], Train Loss: 35.0503, Test Loss: 30.2197, Test Acc: 69.78%\n",
      "Epoch [10/16], Train Loss: 35.0503, Test Loss: 30.2197, Test Acc: 69.78%\n",
      "Epoch [11/16], Train Loss: 35.0503, Test Loss: 30.2197, Test Acc: 69.78%\n",
      "Epoch [12/16], Train Loss: 35.0503, Test Loss: 30.2197, Test Acc: 69.78%\n",
      "Epoch [13/16], Train Loss: 35.0503, Test Loss: 30.2197, Test Acc: 69.78%\n",
      "Epoch [14/16], Train Loss: 35.0503, Test Loss: 30.2197, Test Acc: 69.78%\n",
      "Epoch [15/16], Train Loss: 35.0503, Test Loss: 30.2197, Test Acc: 69.78%\n",
      "Epoch [16/16], Train Loss: 35.0503, Test Loss: 30.2197, Test Acc: 69.78%\n",
      "End: 0.6978027925100119\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 8, 'hidden_size': 64, 'lr': 0.1, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Epoch [1/16], Train Loss: 0.3712, Test Loss: 0.1335, Test Acc: 96.89%\n",
      "Epoch [2/16], Train Loss: 0.3165, Test Loss: 0.2769, Test Acc: 89.37%\n",
      "Epoch [3/16], Train Loss: 0.3044, Test Loss: 0.4346, Test Acc: 80.12%\n",
      "Epoch [4/16], Train Loss: 0.2973, Test Loss: 0.1249, Test Acc: 96.75%\n",
      "Epoch [5/16], Train Loss: 0.2916, Test Loss: 0.1298, Test Acc: 96.86%\n",
      "Epoch [6/16], Train Loss: 0.2880, Test Loss: 0.1839, Test Acc: 94.64%\n",
      "Epoch [7/16], Train Loss: 0.2834, Test Loss: 0.1732, Test Acc: 94.87%\n",
      "Epoch [8/16], Train Loss: 0.2808, Test Loss: 0.1774, Test Acc: 93.57%\n",
      "Epoch [9/16], Train Loss: 0.2787, Test Loss: 0.1603, Test Acc: 94.78%\n",
      "Epoch [10/16], Train Loss: 0.2762, Test Loss: 0.2193, Test Acc: 92.19%\n",
      "Epoch [11/16], Train Loss: 0.2728, Test Loss: 0.1784, Test Acc: 94.86%\n",
      "Epoch [12/16], Train Loss: 0.2707, Test Loss: 0.1447, Test Acc: 95.90%\n",
      "Epoch [13/16], Train Loss: 0.2691, Test Loss: 0.2181, Test Acc: 90.55%\n",
      "Epoch [14/16], Train Loss: 0.2667, Test Loss: 0.1779, Test Acc: 94.40%\n",
      "Epoch [15/16], Train Loss: 0.2649, Test Loss: 0.1413, Test Acc: 96.32%\n",
      "Epoch [16/16], Train Loss: 0.2632, Test Loss: 0.1257, Test Acc: 96.57%\n",
      "End: 0.9656889273730923\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 8, 'hidden_size': 128, 'lr': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Epoch [1/16], Train Loss: 0.3117, Test Loss: 0.1790, Test Acc: 94.60%\n",
      "Epoch [2/16], Train Loss: 0.2790, Test Loss: 0.1757, Test Acc: 95.02%\n",
      "Epoch [3/16], Train Loss: 0.2682, Test Loss: 0.1938, Test Acc: 93.01%\n",
      "Epoch [4/16], Train Loss: 0.2619, Test Loss: 0.1854, Test Acc: 94.10%\n",
      "Epoch [5/16], Train Loss: 0.2560, Test Loss: 0.1596, Test Acc: 94.40%\n",
      "Epoch [6/16], Train Loss: 0.2503, Test Loss: 0.1697, Test Acc: 94.06%\n",
      "Epoch [7/16], Train Loss: 0.2451, Test Loss: 0.1710, Test Acc: 94.11%\n",
      "Epoch [8/16], Train Loss: 0.2415, Test Loss: 0.1883, Test Acc: 93.24%\n",
      "Epoch [9/16], Train Loss: 0.2381, Test Loss: 0.1916, Test Acc: 92.90%\n",
      "Epoch [10/16], Train Loss: 0.2335, Test Loss: 0.2476, Test Acc: 93.06%\n",
      "Epoch [11/16], Train Loss: 0.2284, Test Loss: 0.1645, Test Acc: 95.35%\n",
      "Epoch [12/16], Train Loss: 0.2272, Test Loss: 0.1868, Test Acc: 93.47%\n",
      "Epoch [13/16], Train Loss: 0.2218, Test Loss: 0.1836, Test Acc: 93.57%\n",
      "Epoch [14/16], Train Loss: 0.2185, Test Loss: 0.2060, Test Acc: 92.59%\n",
      "Epoch [15/16], Train Loss: 0.2166, Test Loss: 0.2422, Test Acc: 93.84%\n",
      "Epoch [16/16], Train Loss: 0.2132, Test Loss: 0.2222, Test Acc: 91.93%\n",
      "End: 0.9192553306634917\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 8, 'hidden_size': 128, 'lr': 0.001, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Epoch [1/16], Train Loss: 0.6017, Test Loss: 0.4004, Test Acc: 97.63%\n",
      "Epoch [2/16], Train Loss: 0.5127, Test Loss: 0.3016, Test Acc: 97.63%\n",
      "Epoch [3/16], Train Loss: 0.4892, Test Loss: 0.2719, Test Acc: 97.55%\n",
      "Epoch [4/16], Train Loss: 0.4662, Test Loss: 0.2650, Test Acc: 96.64%\n",
      "Epoch [5/16], Train Loss: 0.4546, Test Loss: 0.2516, Test Acc: 96.35%\n",
      "Epoch [6/16], Train Loss: 0.4471, Test Loss: 0.2376, Test Acc: 96.34%\n",
      "Epoch [7/16], Train Loss: 0.4384, Test Loss: 0.2433, Test Acc: 96.27%\n",
      "Epoch [8/16], Train Loss: 0.4284, Test Loss: 0.2335, Test Acc: 96.51%\n",
      "Epoch [9/16], Train Loss: 0.4207, Test Loss: 0.2306, Test Acc: 96.64%\n",
      "Epoch [10/16], Train Loss: 0.4183, Test Loss: 0.2317, Test Acc: 96.45%\n",
      "Epoch [11/16], Train Loss: 0.4125, Test Loss: 0.2340, Test Acc: 96.49%\n",
      "Epoch [12/16], Train Loss: 0.4046, Test Loss: 0.2266, Test Acc: 96.59%\n",
      "Epoch [13/16], Train Loss: 0.3956, Test Loss: 0.2420, Test Acc: 96.19%\n",
      "Epoch [14/16], Train Loss: 0.3867, Test Loss: 0.2015, Test Acc: 97.06%\n",
      "Epoch [15/16], Train Loss: 0.3736, Test Loss: 0.2399, Test Acc: 95.82%\n",
      "Epoch [16/16], Train Loss: 0.3603, Test Loss: 0.1966, Test Acc: 96.79%\n",
      "End: 0.9678536638164303\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 8, 'hidden_size': 128, 'lr': 0.01, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Epoch [1/16], Train Loss: 0.4476, Test Loss: 0.2918, Test Acc: 96.03%\n",
      "Epoch [2/16], Train Loss: 16.6968, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [3/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [4/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [5/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [6/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [7/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [8/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [9/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [10/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [11/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [12/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [13/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [14/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [15/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [16/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "End: 0.9762961359454486\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 8, 'hidden_size': 128, 'lr': 0.01, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Epoch [1/16], Train Loss: 0.4710, Test Loss: 0.2231, Test Acc: 96.83%\n",
      "Epoch [2/16], Train Loss: 0.3872, Test Loss: 0.1575, Test Acc: 97.16%\n",
      "Epoch [3/16], Train Loss: 0.3334, Test Loss: 0.1233, Test Acc: 97.27%\n",
      "Epoch [4/16], Train Loss: 0.3119, Test Loss: 0.2077, Test Acc: 94.01%\n",
      "Epoch [5/16], Train Loss: 0.3015, Test Loss: 0.1591, Test Acc: 96.26%\n",
      "Epoch [6/16], Train Loss: 0.2936, Test Loss: 0.1438, Test Acc: 96.18%\n",
      "Epoch [7/16], Train Loss: 0.2880, Test Loss: 0.1530, Test Acc: 95.42%\n",
      "Epoch [8/16], Train Loss: 0.2845, Test Loss: 0.1815, Test Acc: 93.88%\n",
      "Epoch [9/16], Train Loss: 0.2817, Test Loss: 0.1764, Test Acc: 94.50%\n",
      "Epoch [10/16], Train Loss: 0.2793, Test Loss: 0.1141, Test Acc: 96.98%\n",
      "Epoch [11/16], Train Loss: 0.2762, Test Loss: 0.1683, Test Acc: 95.11%\n",
      "Epoch [12/16], Train Loss: 0.2743, Test Loss: 0.1398, Test Acc: 95.66%\n",
      "Epoch [13/16], Train Loss: 0.2709, Test Loss: 0.1695, Test Acc: 94.90%\n",
      "Epoch [14/16], Train Loss: 0.2697, Test Loss: 0.2091, Test Acc: 92.19%\n",
      "Epoch [15/16], Train Loss: 0.2674, Test Loss: 0.1617, Test Acc: 95.24%\n",
      "Epoch [16/16], Train Loss: 0.2650, Test Loss: 0.1239, Test Acc: 96.66%\n",
      "End: 0.9665548219504275\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 8, 'hidden_size': 128, 'lr': 0.1, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Epoch [1/16], Train Loss: 19.9912, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [2/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [3/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [4/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [5/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [6/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [7/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [8/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [9/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [10/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [11/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [12/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [13/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [14/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [15/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [16/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "End: 0.9762961359454486\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 8, 'hidden_size': 128, 'lr': 0.1, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Epoch [1/16], Train Loss: 0.3652, Test Loss: 0.2826, Test Acc: 91.58%\n",
      "Epoch [2/16], Train Loss: 0.3143, Test Loss: 0.1690, Test Acc: 95.57%\n",
      "Epoch [3/16], Train Loss: 0.3028, Test Loss: 0.1710, Test Acc: 94.49%\n",
      "Epoch [4/16], Train Loss: 0.2942, Test Loss: 0.1748, Test Acc: 94.92%\n",
      "Epoch [5/16], Train Loss: 0.2885, Test Loss: 0.1845, Test Acc: 94.89%\n",
      "Epoch [6/16], Train Loss: 0.2840, Test Loss: 0.1974, Test Acc: 93.87%\n",
      "Epoch [7/16], Train Loss: 0.2801, Test Loss: 0.1531, Test Acc: 95.80%\n",
      "Epoch [8/16], Train Loss: 0.2771, Test Loss: 0.2592, Test Acc: 89.57%\n",
      "Epoch [9/16], Train Loss: 0.2744, Test Loss: 0.1476, Test Acc: 95.70%\n",
      "Epoch [10/16], Train Loss: 0.2703, Test Loss: 0.1273, Test Acc: 96.71%\n",
      "Epoch [11/16], Train Loss: 0.2697, Test Loss: 0.1606, Test Acc: 94.32%\n",
      "Epoch [12/16], Train Loss: 0.2659, Test Loss: 0.1986, Test Acc: 92.64%\n",
      "Epoch [13/16], Train Loss: 0.2633, Test Loss: 0.2104, Test Acc: 91.87%\n",
      "Epoch [14/16], Train Loss: 0.2618, Test Loss: 0.1297, Test Acc: 96.61%\n",
      "Epoch [15/16], Train Loss: 0.2581, Test Loss: 0.1294, Test Acc: 96.77%\n",
      "Epoch [16/16], Train Loss: 0.2571, Test Loss: 0.1766, Test Acc: 94.27%\n",
      "End: 0.9427427210737093\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 8, 'hidden_size': 256, 'lr': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Epoch [1/16], Train Loss: 0.3163, Test Loss: 0.1802, Test Acc: 94.85%\n",
      "Epoch [2/16], Train Loss: 0.2815, Test Loss: 0.1420, Test Acc: 96.03%\n",
      "Epoch [3/16], Train Loss: 0.2743, Test Loss: 0.1804, Test Acc: 94.54%\n",
      "Epoch [4/16], Train Loss: 0.2706, Test Loss: 0.1728, Test Acc: 94.82%\n",
      "Epoch [5/16], Train Loss: 0.2746, Test Loss: 0.1845, Test Acc: 94.58%\n",
      "Epoch [6/16], Train Loss: 0.2995, Test Loss: 0.1633, Test Acc: 95.93%\n",
      "Epoch [7/16], Train Loss: 0.2635, Test Loss: 0.1725, Test Acc: 95.14%\n",
      "Epoch [8/16], Train Loss: 0.2585, Test Loss: 0.2266, Test Acc: 92.53%\n",
      "Epoch [9/16], Train Loss: 0.2641, Test Loss: 0.2110, Test Acc: 90.46%\n",
      "Epoch [10/16], Train Loss: 0.2524, Test Loss: 0.2059, Test Acc: 94.30%\n",
      "Epoch [11/16], Train Loss: 0.2463, Test Loss: 0.2210, Test Acc: 91.84%\n",
      "Epoch [12/16], Train Loss: 0.2621, Test Loss: 0.2199, Test Acc: 92.93%\n",
      "Epoch [13/16], Train Loss: 0.2404, Test Loss: 0.1945, Test Acc: 94.75%\n",
      "Epoch [14/16], Train Loss: 0.2397, Test Loss: 0.2304, Test Acc: 93.07%\n",
      "Epoch [15/16], Train Loss: 0.2472, Test Loss: 0.2192, Test Acc: 94.54%\n",
      "Epoch [16/16], Train Loss: 0.2730, Test Loss: 0.3160, Test Acc: 90.99%\n",
      "End: 0.9099469639571383\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 8, 'hidden_size': 256, 'lr': 0.001, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Epoch [1/16], Train Loss: 0.5898, Test Loss: 0.3918, Test Acc: 97.63%\n",
      "Epoch [2/16], Train Loss: 0.5116, Test Loss: 0.3007, Test Acc: 97.63%\n",
      "Epoch [3/16], Train Loss: 0.4881, Test Loss: 0.2741, Test Acc: 97.46%\n",
      "Epoch [4/16], Train Loss: 0.4659, Test Loss: 0.2627, Test Acc: 96.71%\n",
      "Epoch [5/16], Train Loss: 0.4548, Test Loss: 0.2520, Test Acc: 96.37%\n",
      "Epoch [6/16], Train Loss: 0.4452, Test Loss: 0.2391, Test Acc: 96.35%\n",
      "Epoch [7/16], Train Loss: 0.4345, Test Loss: 0.2379, Test Acc: 96.51%\n",
      "Epoch [8/16], Train Loss: 0.4208, Test Loss: 0.2332, Test Acc: 96.63%\n",
      "Epoch [9/16], Train Loss: 0.4149, Test Loss: 0.2297, Test Acc: 96.38%\n",
      "Epoch [10/16], Train Loss: 0.4075, Test Loss: 0.2279, Test Acc: 96.53%\n",
      "Epoch [11/16], Train Loss: 0.3972, Test Loss: 0.2496, Test Acc: 95.94%\n",
      "Epoch [12/16], Train Loss: 0.3867, Test Loss: 0.2444, Test Acc: 95.80%\n",
      "Epoch [13/16], Train Loss: 0.3743, Test Loss: 0.2337, Test Acc: 95.89%\n",
      "Epoch [14/16], Train Loss: 0.3634, Test Loss: 0.1991, Test Acc: 96.75%\n",
      "Epoch [15/16], Train Loss: 0.3472, Test Loss: 0.2629, Test Acc: 94.19%\n",
      "Epoch [16/16], Train Loss: 0.3361, Test Loss: 0.2307, Test Acc: 94.84%\n",
      "End: 0.9483710358263882\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 8, 'hidden_size': 256, 'lr': 0.01, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Epoch [1/16], Train Loss: 15.2658, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [2/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [3/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [4/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [5/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [6/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [7/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [8/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [9/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [10/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [11/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [12/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [13/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [14/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [15/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [16/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "End: 0.9762961359454486\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 8, 'hidden_size': 256, 'lr': 0.01, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Epoch [1/16], Train Loss: 0.4568, Test Loss: 0.2188, Test Acc: 96.63%\n",
      "Epoch [2/16], Train Loss: 0.3706, Test Loss: 0.2159, Test Acc: 95.05%\n",
      "Epoch [3/16], Train Loss: 0.3234, Test Loss: 0.3772, Test Acc: 83.63%\n",
      "Epoch [4/16], Train Loss: 0.3031, Test Loss: 0.1410, Test Acc: 96.70%\n",
      "Epoch [5/16], Train Loss: 0.2943, Test Loss: 0.1369, Test Acc: 96.33%\n",
      "Epoch [6/16], Train Loss: 0.2865, Test Loss: 0.1642, Test Acc: 95.68%\n",
      "Epoch [7/16], Train Loss: 0.2819, Test Loss: 0.1112, Test Acc: 96.90%\n",
      "Epoch [8/16], Train Loss: 0.2774, Test Loss: 0.1918, Test Acc: 93.09%\n",
      "Epoch [9/16], Train Loss: 0.2737, Test Loss: 0.1159, Test Acc: 96.92%\n",
      "Epoch [10/16], Train Loss: 0.2704, Test Loss: 0.1422, Test Acc: 95.74%\n",
      "Epoch [11/16], Train Loss: 0.2673, Test Loss: 0.1642, Test Acc: 95.27%\n",
      "Epoch [12/16], Train Loss: 0.2646, Test Loss: 0.1765, Test Acc: 94.29%\n",
      "Epoch [13/16], Train Loss: 0.2615, Test Loss: 0.1934, Test Acc: 93.61%\n",
      "Epoch [14/16], Train Loss: 0.2589, Test Loss: 0.1211, Test Acc: 96.74%\n",
      "Epoch [15/16], Train Loss: 0.2568, Test Loss: 0.1470, Test Acc: 95.74%\n",
      "Epoch [16/16], Train Loss: 0.2545, Test Loss: 0.2168, Test Acc: 90.86%\n",
      "End: 0.9086481220911354\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 8, 'hidden_size': 256, 'lr': 0.1, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Epoch [1/16], Train Loss: 19.9934, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [2/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [3/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [4/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [5/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [6/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [7/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [8/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [9/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [10/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [11/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [12/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [13/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [14/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [15/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [16/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "End: 0.9762961359454486\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 8, 'hidden_size': 256, 'lr': 0.1, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Epoch [1/16], Train Loss: 0.3603, Test Loss: 0.1070, Test Acc: 97.20%\n",
      "Epoch [2/16], Train Loss: 0.3116, Test Loss: 0.1703, Test Acc: 95.22%\n",
      "Epoch [3/16], Train Loss: 0.2993, Test Loss: 0.1336, Test Acc: 96.23%\n",
      "Epoch [4/16], Train Loss: 0.2920, Test Loss: 0.1076, Test Acc: 97.14%\n",
      "Epoch [5/16], Train Loss: 0.2856, Test Loss: 0.1746, Test Acc: 95.77%\n",
      "Epoch [6/16], Train Loss: 0.2812, Test Loss: 0.1781, Test Acc: 94.63%\n",
      "Epoch [7/16], Train Loss: 0.2766, Test Loss: 0.1661, Test Acc: 96.40%\n",
      "Epoch [8/16], Train Loss: 0.2717, Test Loss: 0.2207, Test Acc: 90.66%\n",
      "Epoch [9/16], Train Loss: 0.2685, Test Loss: 0.2069, Test Acc: 93.42%\n",
      "Epoch [10/16], Train Loss: 0.2653, Test Loss: 0.1446, Test Acc: 96.53%\n",
      "Epoch [11/16], Train Loss: 0.2628, Test Loss: 0.1460, Test Acc: 94.97%\n",
      "Epoch [12/16], Train Loss: 0.2588, Test Loss: 0.1636, Test Acc: 95.19%\n",
      "Epoch [13/16], Train Loss: 0.2567, Test Loss: 0.1605, Test Acc: 95.81%\n",
      "Epoch [14/16], Train Loss: 0.2547, Test Loss: 0.1495, Test Acc: 95.64%\n",
      "Epoch [15/16], Train Loss: 0.2517, Test Loss: 0.2699, Test Acc: 88.34%\n",
      "Epoch [16/16], Train Loss: 0.2487, Test Loss: 0.1968, Test Acc: 93.88%\n",
      "End: 0.9388461954757008\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 10, 'hidden_size': 64, 'lr': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Epoch [1/16], Train Loss: 0.3122, Test Loss: 0.1840, Test Acc: 93.61%\n",
      "Epoch [2/16], Train Loss: 0.2791, Test Loss: 0.2542, Test Acc: 92.12%\n",
      "Epoch [3/16], Train Loss: 0.2714, Test Loss: 0.1581, Test Acc: 95.44%\n",
      "Epoch [4/16], Train Loss: 0.2655, Test Loss: 0.1736, Test Acc: 94.34%\n",
      "Epoch [5/16], Train Loss: 0.2601, Test Loss: 0.1819, Test Acc: 95.14%\n",
      "Epoch [6/16], Train Loss: 0.2548, Test Loss: 0.1974, Test Acc: 93.66%\n",
      "Epoch [7/16], Train Loss: 0.2503, Test Loss: 0.1686, Test Acc: 95.08%\n",
      "Epoch [8/16], Train Loss: 0.2464, Test Loss: 0.1700, Test Acc: 94.53%\n",
      "Epoch [9/16], Train Loss: 0.2425, Test Loss: 0.1834, Test Acc: 93.71%\n",
      "Epoch [10/16], Train Loss: 0.2392, Test Loss: 0.2052, Test Acc: 91.73%\n",
      "Epoch [11/16], Train Loss: 0.2356, Test Loss: 0.2001, Test Acc: 92.29%\n",
      "Epoch [12/16], Train Loss: 0.2330, Test Loss: 0.1900, Test Acc: 93.67%\n",
      "Epoch [13/16], Train Loss: 0.2294, Test Loss: 0.2112, Test Acc: 92.68%\n",
      "Epoch [14/16], Train Loss: 0.2274, Test Loss: 0.2093, Test Acc: 92.46%\n",
      "Epoch [15/16], Train Loss: 0.2242, Test Loss: 0.2023, Test Acc: 93.53%\n",
      "Epoch [16/16], Train Loss: 0.2219, Test Loss: 0.2019, Test Acc: 94.63%\n",
      "End: 0.946314536205217\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 10, 'hidden_size': 64, 'lr': 0.001, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Epoch [1/16], Train Loss: 0.6019, Test Loss: 0.4010, Test Acc: 97.63%\n",
      "Epoch [2/16], Train Loss: 0.5152, Test Loss: 0.2997, Test Acc: 97.63%\n",
      "Epoch [3/16], Train Loss: 0.5019, Test Loss: 0.2695, Test Acc: 97.63%\n",
      "Epoch [4/16], Train Loss: 0.5001, Test Loss: 0.2597, Test Acc: 97.63%\n",
      "Epoch [5/16], Train Loss: 0.4996, Test Loss: 0.2568, Test Acc: 97.63%\n",
      "Epoch [6/16], Train Loss: 0.4992, Test Loss: 0.2553, Test Acc: 97.63%\n",
      "Epoch [7/16], Train Loss: 0.4985, Test Loss: 0.2542, Test Acc: 97.63%\n",
      "Epoch [8/16], Train Loss: 0.4974, Test Loss: 0.2541, Test Acc: 97.63%\n",
      "Epoch [9/16], Train Loss: 0.4953, Test Loss: 0.2528, Test Acc: 97.63%\n",
      "Epoch [10/16], Train Loss: 0.4909, Test Loss: 0.2504, Test Acc: 97.63%\n",
      "Epoch [11/16], Train Loss: 0.4818, Test Loss: 0.2486, Test Acc: 97.61%\n",
      "Epoch [12/16], Train Loss: 0.4667, Test Loss: 0.2513, Test Acc: 97.13%\n",
      "Epoch [13/16], Train Loss: 0.4547, Test Loss: 0.2464, Test Acc: 96.48%\n",
      "Epoch [14/16], Train Loss: 0.4495, Test Loss: 0.2414, Test Acc: 96.37%\n",
      "Epoch [15/16], Train Loss: 0.4461, Test Loss: 0.2340, Test Acc: 96.36%\n",
      "Epoch [16/16], Train Loss: 0.4404, Test Loss: 0.2291, Test Acc: 96.32%\n",
      "End: 0.9631994804632535\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 10, 'hidden_size': 64, 'lr': 0.01, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Epoch [1/16], Train Loss: 43.2149, Test Loss: 97.6296, Test Acc: 2.37%\n",
      "Epoch [2/16], Train Loss: 80.0002, Test Loss: 97.6296, Test Acc: 2.37%\n",
      "Epoch [3/16], Train Loss: 80.0002, Test Loss: 97.6296, Test Acc: 2.37%\n",
      "Epoch [4/16], Train Loss: 80.0002, Test Loss: 97.6296, Test Acc: 2.37%\n",
      "Epoch [5/16], Train Loss: 80.0002, Test Loss: 97.6296, Test Acc: 2.37%\n",
      "Epoch [6/16], Train Loss: 80.0002, Test Loss: 97.6296, Test Acc: 2.37%\n",
      "Epoch [7/16], Train Loss: 80.0002, Test Loss: 97.6296, Test Acc: 2.37%\n",
      "Epoch [8/16], Train Loss: 80.0002, Test Loss: 97.6296, Test Acc: 2.37%\n",
      "Epoch [9/16], Train Loss: 80.0002, Test Loss: 97.6296, Test Acc: 2.37%\n",
      "Epoch [10/16], Train Loss: 80.0002, Test Loss: 97.6296, Test Acc: 2.37%\n",
      "Epoch [11/16], Train Loss: 80.0002, Test Loss: 97.6296, Test Acc: 2.37%\n",
      "Epoch [12/16], Train Loss: 80.0002, Test Loss: 97.6296, Test Acc: 2.37%\n",
      "Epoch [13/16], Train Loss: 80.0002, Test Loss: 97.6296, Test Acc: 2.37%\n",
      "Epoch [14/16], Train Loss: 80.0002, Test Loss: 97.6296, Test Acc: 2.37%\n",
      "Epoch [15/16], Train Loss: 80.0002, Test Loss: 97.6296, Test Acc: 2.37%\n",
      "Epoch [16/16], Train Loss: 80.0002, Test Loss: 97.6296, Test Acc: 2.37%\n",
      "End: 0.023703864054551358\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 10, 'hidden_size': 64, 'lr': 0.01, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Epoch [1/16], Train Loss: 0.5093, Test Loss: 0.2554, Test Acc: 97.63%\n",
      "Epoch [2/16], Train Loss: 0.4712, Test Loss: 0.2283, Test Acc: 96.44%\n",
      "Epoch [3/16], Train Loss: 0.4016, Test Loss: 0.1728, Test Acc: 96.96%\n",
      "Epoch [4/16], Train Loss: 0.3490, Test Loss: 0.1341, Test Acc: 96.63%\n",
      "Epoch [5/16], Train Loss: 0.3208, Test Loss: 0.3685, Test Acc: 83.27%\n",
      "Epoch [6/16], Train Loss: 0.3047, Test Loss: 0.2111, Test Acc: 94.36%\n",
      "Epoch [7/16], Train Loss: 0.2962, Test Loss: 0.1812, Test Acc: 94.84%\n",
      "Epoch [8/16], Train Loss: 0.2907, Test Loss: 0.1224, Test Acc: 96.88%\n",
      "Epoch [9/16], Train Loss: 0.2875, Test Loss: 0.1322, Test Acc: 96.57%\n",
      "Epoch [10/16], Train Loss: 0.2839, Test Loss: 0.1146, Test Acc: 96.89%\n",
      "Epoch [11/16], Train Loss: 0.2816, Test Loss: 0.2867, Test Acc: 88.39%\n",
      "Epoch [12/16], Train Loss: 0.2778, Test Loss: 0.1698, Test Acc: 95.39%\n",
      "Epoch [13/16], Train Loss: 0.2772, Test Loss: 0.2298, Test Acc: 92.29%\n",
      "Epoch [14/16], Train Loss: 0.2742, Test Loss: 0.2142, Test Acc: 91.69%\n",
      "Epoch [15/16], Train Loss: 0.2728, Test Loss: 0.1346, Test Acc: 96.58%\n",
      "Epoch [16/16], Train Loss: 0.2701, Test Loss: 0.4013, Test Acc: 82.40%\n",
      "End: 0.8240069271566187\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 10, 'hidden_size': 64, 'lr': 0.1, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Epoch [1/16], Train Loss: 19.9923, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [2/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [3/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [4/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [5/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [6/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [7/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [8/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [9/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [10/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [11/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [12/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [13/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [14/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [15/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [16/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "End: 0.9762961359454486\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 10, 'hidden_size': 64, 'lr': 0.1, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Epoch [1/16], Train Loss: 0.3867, Test Loss: 0.1695, Test Acc: 95.61%\n",
      "Epoch [2/16], Train Loss: 0.3231, Test Loss: 0.4087, Test Acc: 84.63%\n",
      "Epoch [3/16], Train Loss: 0.3088, Test Loss: 0.1820, Test Acc: 94.96%\n",
      "Epoch [4/16], Train Loss: 0.3015, Test Loss: 0.1272, Test Acc: 96.72%\n",
      "Epoch [5/16], Train Loss: 0.2960, Test Loss: 0.2154, Test Acc: 94.86%\n",
      "Epoch [6/16], Train Loss: 0.2925, Test Loss: 0.1508, Test Acc: 96.49%\n",
      "Epoch [7/16], Train Loss: 0.2882, Test Loss: 0.1197, Test Acc: 97.38%\n",
      "Epoch [8/16], Train Loss: 0.2863, Test Loss: 0.1704, Test Acc: 95.83%\n",
      "Epoch [9/16], Train Loss: 0.2832, Test Loss: 0.1551, Test Acc: 96.47%\n",
      "Epoch [10/16], Train Loss: 0.2804, Test Loss: 0.1569, Test Acc: 95.56%\n",
      "Epoch [11/16], Train Loss: 0.2778, Test Loss: 0.1429, Test Acc: 95.89%\n",
      "Epoch [12/16], Train Loss: 0.2761, Test Loss: 0.1688, Test Acc: 96.11%\n",
      "Epoch [13/16], Train Loss: 0.2745, Test Loss: 0.1650, Test Acc: 94.90%\n",
      "Epoch [14/16], Train Loss: 0.2716, Test Loss: 0.1503, Test Acc: 96.10%\n",
      "Epoch [15/16], Train Loss: 0.2706, Test Loss: 0.1480, Test Acc: 95.53%\n",
      "Epoch [16/16], Train Loss: 0.2686, Test Loss: 0.1979, Test Acc: 92.13%\n",
      "End: 0.9213118302846628\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 10, 'hidden_size': 128, 'lr': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Epoch [1/16], Train Loss: 0.3127, Test Loss: 0.2107, Test Acc: 93.05%\n",
      "Epoch [2/16], Train Loss: 0.2783, Test Loss: 0.2119, Test Acc: 93.09%\n",
      "Epoch [3/16], Train Loss: 0.2700, Test Loss: 0.2040, Test Acc: 92.57%\n",
      "Epoch [4/16], Train Loss: 0.2635, Test Loss: 0.1468, Test Acc: 95.62%\n",
      "Epoch [5/16], Train Loss: 0.2598, Test Loss: 0.2049, Test Acc: 92.04%\n",
      "Epoch [6/16], Train Loss: 0.2558, Test Loss: 0.1511, Test Acc: 96.14%\n",
      "Epoch [7/16], Train Loss: 0.2501, Test Loss: 0.1917, Test Acc: 93.13%\n",
      "Epoch [8/16], Train Loss: 0.2464, Test Loss: 0.2213, Test Acc: 94.04%\n",
      "Epoch [9/16], Train Loss: 0.2435, Test Loss: 0.1663, Test Acc: 95.10%\n",
      "Epoch [10/16], Train Loss: 0.2412, Test Loss: 0.1746, Test Acc: 94.83%\n",
      "Epoch [11/16], Train Loss: 0.2486, Test Loss: 0.2263, Test Acc: 91.76%\n",
      "Epoch [12/16], Train Loss: 0.2356, Test Loss: 0.2163, Test Acc: 93.45%\n",
      "Epoch [13/16], Train Loss: 0.2374, Test Loss: 0.2257, Test Acc: 93.64%\n",
      "Epoch [14/16], Train Loss: 0.2355, Test Loss: 0.2305, Test Acc: 94.43%\n",
      "Epoch [15/16], Train Loss: 0.2365, Test Loss: 0.2264, Test Acc: 94.66%\n",
      "Epoch [16/16], Train Loss: 0.2292, Test Loss: 0.3000, Test Acc: 94.11%\n",
      "End: 0.9411191687412057\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 10, 'hidden_size': 128, 'lr': 0.001, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Epoch [1/16], Train Loss: 0.6014, Test Loss: 0.3932, Test Acc: 97.63%\n",
      "Epoch [2/16], Train Loss: 0.5131, Test Loss: 0.2944, Test Acc: 97.63%\n",
      "Epoch [3/16], Train Loss: 0.5011, Test Loss: 0.2665, Test Acc: 97.63%\n",
      "Epoch [4/16], Train Loss: 0.4993, Test Loss: 0.2591, Test Acc: 97.63%\n",
      "Epoch [5/16], Train Loss: 0.4980, Test Loss: 0.2551, Test Acc: 97.63%\n",
      "Epoch [6/16], Train Loss: 0.4957, Test Loss: 0.2523, Test Acc: 97.63%\n",
      "Epoch [7/16], Train Loss: 0.4908, Test Loss: 0.2508, Test Acc: 97.63%\n",
      "Epoch [8/16], Train Loss: 0.4804, Test Loss: 0.2504, Test Acc: 97.59%\n",
      "Epoch [9/16], Train Loss: 0.4646, Test Loss: 0.2515, Test Acc: 97.02%\n",
      "Epoch [10/16], Train Loss: 0.4542, Test Loss: 0.2425, Test Acc: 96.49%\n",
      "Epoch [11/16], Train Loss: 0.4476, Test Loss: 0.2343, Test Acc: 96.43%\n",
      "Epoch [12/16], Train Loss: 0.4395, Test Loss: 0.2289, Test Acc: 96.38%\n",
      "Epoch [13/16], Train Loss: 0.4254, Test Loss: 0.2277, Test Acc: 96.60%\n",
      "Epoch [14/16], Train Loss: 0.4139, Test Loss: 0.2259, Test Acc: 96.48%\n",
      "Epoch [15/16], Train Loss: 0.4074, Test Loss: 0.2233, Test Acc: 96.30%\n",
      "Epoch [16/16], Train Loss: 0.4017, Test Loss: 0.1972, Test Acc: 96.73%\n",
      "End: 0.9673124797055959\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 10, 'hidden_size': 128, 'lr': 0.01, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Epoch [1/16], Train Loss: 0.3880, Test Loss: 0.2060, Test Acc: 94.65%\n",
      "Epoch [2/16], Train Loss: 0.3484, Test Loss: 0.1740, Test Acc: 97.25%\n",
      "Epoch [3/16], Train Loss: 0.4295, Test Loss: 0.2149, Test Acc: 97.39%\n",
      "Epoch [4/16], Train Loss: 0.7905, Test Loss: 0.2170, Test Acc: 96.73%\n",
      "Epoch [5/16], Train Loss: 0.4065, Test Loss: 0.1971, Test Acc: 96.73%\n",
      "Epoch [6/16], Train Loss: 0.4064, Test Loss: 0.2242, Test Acc: 96.73%\n",
      "Epoch [7/16], Train Loss: 0.4062, Test Loss: 0.1923, Test Acc: 96.73%\n",
      "Epoch [8/16], Train Loss: 0.4064, Test Loss: 0.1941, Test Acc: 96.73%\n",
      "Epoch [9/16], Train Loss: 0.4064, Test Loss: 0.2350, Test Acc: 96.73%\n",
      "Epoch [10/16], Train Loss: 0.4061, Test Loss: 0.2177, Test Acc: 96.73%\n",
      "Epoch [11/16], Train Loss: 0.4062, Test Loss: 0.2399, Test Acc: 96.73%\n",
      "Epoch [12/16], Train Loss: 0.4063, Test Loss: 0.2068, Test Acc: 96.73%\n",
      "Epoch [13/16], Train Loss: 0.4063, Test Loss: 0.2075, Test Acc: 96.73%\n",
      "Epoch [14/16], Train Loss: 0.4063, Test Loss: 0.2058, Test Acc: 96.73%\n",
      "Epoch [15/16], Train Loss: 0.4062, Test Loss: 0.2036, Test Acc: 96.73%\n",
      "Epoch [16/16], Train Loss: 0.4063, Test Loss: 0.2248, Test Acc: 96.73%\n",
      "End: 0.9673124797055959\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 10, 'hidden_size': 128, 'lr': 0.01, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Epoch [1/16], Train Loss: 0.4995, Test Loss: 0.2354, Test Acc: 96.54%\n",
      "Epoch [2/16], Train Loss: 0.4167, Test Loss: 0.2444, Test Acc: 95.45%\n",
      "Epoch [3/16], Train Loss: 0.3484, Test Loss: 0.1901, Test Acc: 94.70%\n",
      "Epoch [4/16], Train Loss: 0.3199, Test Loss: 0.1643, Test Acc: 95.59%\n",
      "Epoch [5/16], Train Loss: 0.3043, Test Loss: 0.2632, Test Acc: 90.76%\n",
      "Epoch [6/16], Train Loss: 0.2950, Test Loss: 0.2381, Test Acc: 92.34%\n",
      "Epoch [7/16], Train Loss: 0.2899, Test Loss: 0.1721, Test Acc: 95.10%\n",
      "Epoch [8/16], Train Loss: 0.2851, Test Loss: 0.1350, Test Acc: 96.40%\n",
      "Epoch [9/16], Train Loss: 0.2813, Test Loss: 0.2309, Test Acc: 90.89%\n",
      "Epoch [10/16], Train Loss: 0.2779, Test Loss: 0.1845, Test Acc: 94.36%\n",
      "Epoch [11/16], Train Loss: 0.2740, Test Loss: 0.1760, Test Acc: 94.35%\n",
      "Epoch [12/16], Train Loss: 0.2714, Test Loss: 0.1581, Test Acc: 95.32%\n",
      "Epoch [13/16], Train Loss: 0.2687, Test Loss: 0.1620, Test Acc: 94.73%\n",
      "Epoch [14/16], Train Loss: 0.2664, Test Loss: 0.1527, Test Acc: 96.01%\n",
      "Epoch [15/16], Train Loss: 0.2639, Test Loss: 0.1202, Test Acc: 96.75%\n",
      "Epoch [16/16], Train Loss: 0.2621, Test Loss: 0.1677, Test Acc: 94.60%\n",
      "End: 0.9459898257387163\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 10, 'hidden_size': 128, 'lr': 0.1, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Epoch [1/16], Train Loss: 19.9967, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [2/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [3/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [4/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [5/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [6/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [7/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [8/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [9/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [10/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [11/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [12/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [13/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [14/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [15/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [16/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "End: 0.9762961359454486\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 10, 'hidden_size': 128, 'lr': 0.1, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Epoch [1/16], Train Loss: 0.3804, Test Loss: 0.1435, Test Acc: 96.81%\n",
      "Epoch [2/16], Train Loss: 0.3189, Test Loss: 0.2067, Test Acc: 96.20%\n",
      "Epoch [3/16], Train Loss: 0.3058, Test Loss: 0.1489, Test Acc: 96.02%\n",
      "Epoch [4/16], Train Loss: 0.2977, Test Loss: 0.1637, Test Acc: 96.17%\n",
      "Epoch [5/16], Train Loss: 0.2916, Test Loss: 0.1557, Test Acc: 95.21%\n",
      "Epoch [6/16], Train Loss: 0.2864, Test Loss: 0.1147, Test Acc: 96.92%\n",
      "Epoch [7/16], Train Loss: 0.2831, Test Loss: 0.1768, Test Acc: 94.62%\n",
      "Epoch [8/16], Train Loss: 0.2788, Test Loss: 0.1289, Test Acc: 96.95%\n",
      "Epoch [9/16], Train Loss: 0.2757, Test Loss: 0.1108, Test Acc: 97.22%\n",
      "Epoch [10/16], Train Loss: 0.2729, Test Loss: 0.1531, Test Acc: 96.14%\n",
      "Epoch [11/16], Train Loss: 0.2708, Test Loss: 0.1893, Test Acc: 93.84%\n",
      "Epoch [12/16], Train Loss: 0.2689, Test Loss: 0.1935, Test Acc: 94.38%\n",
      "Epoch [13/16], Train Loss: 0.2662, Test Loss: 0.2630, Test Acc: 87.90%\n",
      "Epoch [14/16], Train Loss: 0.2640, Test Loss: 0.2214, Test Acc: 90.69%\n",
      "Epoch [15/16], Train Loss: 0.2619, Test Loss: 0.1562, Test Acc: 95.15%\n",
      "Epoch [16/16], Train Loss: 0.2593, Test Loss: 0.1605, Test Acc: 95.87%\n",
      "End: 0.9586535339322437\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 10, 'hidden_size': 256, 'lr': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Epoch [1/16], Train Loss: 0.3525, Test Loss: 0.1972, Test Acc: 93.96%\n",
      "Epoch [2/16], Train Loss: 0.4382, Test Loss: 0.1302, Test Acc: 96.45%\n",
      "Epoch [3/16], Train Loss: 0.3798, Test Loss: 0.1881, Test Acc: 94.90%\n",
      "Epoch [4/16], Train Loss: 4.5808, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [5/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [6/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [7/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [8/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [9/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [10/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [11/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [12/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [13/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [14/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [15/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [16/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "End: 0.9762961359454486\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 10, 'hidden_size': 256, 'lr': 0.001, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Epoch [1/16], Train Loss: 0.5850, Test Loss: 0.3813, Test Acc: 97.63%\n",
      "Epoch [2/16], Train Loss: 0.5116, Test Loss: 0.2935, Test Acc: 97.63%\n",
      "Epoch [3/16], Train Loss: 0.5009, Test Loss: 0.2670, Test Acc: 97.63%\n",
      "Epoch [4/16], Train Loss: 0.4989, Test Loss: 0.2582, Test Acc: 97.63%\n",
      "Epoch [5/16], Train Loss: 0.4972, Test Loss: 0.2553, Test Acc: 97.63%\n",
      "Epoch [6/16], Train Loss: 0.4938, Test Loss: 0.2517, Test Acc: 97.63%\n",
      "Epoch [7/16], Train Loss: 0.4863, Test Loss: 0.2497, Test Acc: 97.63%\n",
      "Epoch [8/16], Train Loss: 0.4706, Test Loss: 0.2527, Test Acc: 97.34%\n",
      "Epoch [9/16], Train Loss: 0.4545, Test Loss: 0.2470, Test Acc: 96.66%\n",
      "Epoch [10/16], Train Loss: 0.4476, Test Loss: 0.2414, Test Acc: 96.24%\n",
      "Epoch [11/16], Train Loss: 0.4437, Test Loss: 0.2318, Test Acc: 96.29%\n",
      "Epoch [12/16], Train Loss: 0.4373, Test Loss: 0.2295, Test Acc: 96.18%\n",
      "Epoch [13/16], Train Loss: 0.4271, Test Loss: 0.2301, Test Acc: 96.48%\n",
      "Epoch [14/16], Train Loss: 0.4162, Test Loss: 0.2247, Test Acc: 96.62%\n",
      "Epoch [15/16], Train Loss: 0.4068, Test Loss: 0.2207, Test Acc: 96.58%\n",
      "Epoch [16/16], Train Loss: 0.4018, Test Loss: 0.2277, Test Acc: 96.38%\n",
      "End: 0.963848901396255\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 10, 'hidden_size': 256, 'lr': 0.01, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Epoch [1/16], Train Loss: 14.7415, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [2/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [3/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [4/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [5/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [6/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [7/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [8/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [9/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [10/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [11/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [12/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [13/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [14/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [15/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [16/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "End: 0.9762961359454486\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 10, 'hidden_size': 256, 'lr': 0.01, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Epoch [1/16], Train Loss: 0.4932, Test Loss: 0.2270, Test Acc: 96.48%\n",
      "Epoch [2/16], Train Loss: 0.4066, Test Loss: 0.1779, Test Acc: 95.93%\n",
      "Epoch [3/16], Train Loss: 0.3379, Test Loss: 0.1839, Test Acc: 94.43%\n",
      "Epoch [4/16], Train Loss: 0.3137, Test Loss: 0.1853, Test Acc: 95.64%\n",
      "Epoch [5/16], Train Loss: 0.2993, Test Loss: 0.1570, Test Acc: 95.27%\n",
      "Epoch [6/16], Train Loss: 0.2915, Test Loss: 0.2252, Test Acc: 92.42%\n",
      "Epoch [7/16], Train Loss: 0.2852, Test Loss: 0.1462, Test Acc: 95.76%\n",
      "Epoch [8/16], Train Loss: 0.2812, Test Loss: 0.2490, Test Acc: 90.09%\n",
      "Epoch [9/16], Train Loss: 0.2769, Test Loss: 0.1868, Test Acc: 94.65%\n",
      "Epoch [10/16], Train Loss: 0.2743, Test Loss: 0.1625, Test Acc: 94.95%\n",
      "Epoch [11/16], Train Loss: 0.2710, Test Loss: 0.1318, Test Acc: 96.26%\n",
      "Epoch [12/16], Train Loss: 0.2675, Test Loss: 0.2711, Test Acc: 88.58%\n",
      "Epoch [13/16], Train Loss: 0.2655, Test Loss: 0.1302, Test Acc: 96.28%\n",
      "Epoch [14/16], Train Loss: 0.2619, Test Loss: 0.2558, Test Acc: 88.97%\n",
      "Epoch [15/16], Train Loss: 0.2595, Test Loss: 0.2157, Test Acc: 91.87%\n",
      "Epoch [16/16], Train Loss: 0.2564, Test Loss: 0.1573, Test Acc: 94.89%\n",
      "End: 0.9489122199372226\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 10, 'hidden_size': 256, 'lr': 0.1, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Epoch [1/16], Train Loss: 19.9912, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [2/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [3/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [4/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [5/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [6/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [7/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [8/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [9/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [10/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [11/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [12/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [13/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [14/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [15/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "Epoch [16/16], Train Loss: 19.9998, Test Loss: 2.3704, Test Acc: 97.63%\n",
      "End: 0.9762961359454486\n",
      "Current best: 0.9762961359454486\n",
      "Start {'hidden_layers': 10, 'hidden_size': 256, 'lr': 0.1, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Epoch [1/16], Train Loss: 0.3732, Test Loss: 0.1585, Test Acc: 95.94%\n",
      "Epoch [2/16], Train Loss: 0.3166, Test Loss: 0.1689, Test Acc: 94.96%\n",
      "Epoch [3/16], Train Loss: 0.3007, Test Loss: 0.1239, Test Acc: 96.80%\n",
      "Epoch [4/16], Train Loss: 0.2929, Test Loss: 0.1558, Test Acc: 95.76%\n",
      "Epoch [5/16], Train Loss: 0.2876, Test Loss: 0.1504, Test Acc: 95.27%\n",
      "Epoch [6/16], Train Loss: 0.2814, Test Loss: 0.1785, Test Acc: 94.93%\n",
      "Epoch [7/16], Train Loss: 0.2779, Test Loss: 0.1637, Test Acc: 96.66%\n",
      "Epoch [8/16], Train Loss: 0.2726, Test Loss: 0.2009, Test Acc: 94.51%\n",
      "Epoch [9/16], Train Loss: 0.2700, Test Loss: 0.1685, Test Acc: 96.15%\n",
      "Epoch [10/16], Train Loss: 0.2670, Test Loss: 0.1527, Test Acc: 95.49%\n",
      "Epoch [11/16], Train Loss: 0.2640, Test Loss: 0.1238, Test Acc: 96.67%\n",
      "Epoch [12/16], Train Loss: 0.2615, Test Loss: 0.1570, Test Acc: 94.45%\n",
      "Epoch [13/16], Train Loss: 0.2591, Test Loss: 0.1460, Test Acc: 96.02%\n",
      "Epoch [14/16], Train Loss: 0.2567, Test Loss: 0.1688, Test Acc: 94.88%\n",
      "Epoch [15/16], Train Loss: 0.2538, Test Loss: 0.2344, Test Acc: 90.49%\n",
      "Epoch [16/16], Train Loss: 0.2515, Test Loss: 0.1897, Test Acc: 93.39%\n",
      "End: 0.9338673016560234\n",
      "Current best: 0.9762961359454486\n",
      "Best Accuracy: 97.63%\n",
      "Best Parameters: {'hidden_layers': 5, 'hidden_size': 64, 'lr': 0.1, 'optimizer': <class 'torch.optim.adam.Adam'>}\n"
     ]
    }
   ],
   "source": [
    "# set up parameter grid search\n",
    "best_acc = 0.0\n",
    "best_params = None\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(\"Start\", params)\n",
    "    model = MLP(input_size=X_train.shape[1], output_size=1,\n",
    "                hidden_layers=params['hidden_layers'],\n",
    "                hidden_size=params['hidden_size'])\n",
    "    model.to(device)\n",
    "    optimizer = params['optimizer'](model.parameters(), lr=params['lr'])\n",
    "    criterion = nn.BCELoss()\n",
    "    acc = train_and_eval(model, train_loader, test_loader, optimizer, criterion, epochs=16)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_params = params\n",
    "    print(\"End:\", acc)\n",
    "    print(\"Current best:\", best_acc)\n",
    "\n",
    "print('Best Accuracy: {:.2f}%'.format(best_acc*100))\n",
    "print('Best Parameters:', best_params)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Loss: 0.5986, Test Loss: 0.4106, Test Acc: 97.63%\n",
      "Epoch [2/20], Train Loss: 0.5172, Test Loss: 0.3145, Test Acc: 97.63%\n",
      "Epoch [3/20], Train Loss: 0.4958, Test Loss: 0.2779, Test Acc: 97.62%\n",
      "Epoch [4/20], Train Loss: 0.4776, Test Loss: 0.2691, Test Acc: 97.16%\n",
      "Epoch [5/20], Train Loss: 0.4627, Test Loss: 0.2599, Test Acc: 96.54%\n",
      "Epoch [6/20], Train Loss: 0.4560, Test Loss: 0.2464, Test Acc: 96.53%\n",
      "Epoch [7/20], Train Loss: 0.4510, Test Loss: 0.2425, Test Acc: 96.41%\n",
      "Epoch [8/20], Train Loss: 0.4444, Test Loss: 0.2415, Test Acc: 96.50%\n",
      "Epoch [9/20], Train Loss: 0.4359, Test Loss: 0.2378, Test Acc: 96.54%\n",
      "Epoch [10/20], Train Loss: 0.4247, Test Loss: 0.2290, Test Acc: 96.75%\n",
      "Epoch [11/20], Train Loss: 0.4191, Test Loss: 0.2184, Test Acc: 97.01%\n",
      "Epoch [12/20], Train Loss: 0.4133, Test Loss: 0.2330, Test Acc: 96.38%\n",
      "Epoch [13/20], Train Loss: 0.4039, Test Loss: 0.2329, Test Acc: 96.50%\n",
      "Epoch [14/20], Train Loss: 0.3928, Test Loss: 0.2550, Test Acc: 95.79%\n",
      "Epoch [15/20], Train Loss: 0.3811, Test Loss: 0.2249, Test Acc: 96.44%\n",
      "Epoch [16/20], Train Loss: 0.3670, Test Loss: 0.2053, Test Acc: 96.62%\n",
      "Epoch [17/20], Train Loss: 0.3464, Test Loss: 0.1955, Test Acc: 96.38%\n",
      "Epoch [18/20], Train Loss: 0.3357, Test Loss: 0.1824, Test Acc: 96.43%\n",
      "Epoch [19/20], Train Loss: 0.3279, Test Loss: 0.2548, Test Acc: 92.74%\n",
      "Epoch [20/20], Train Loss: 0.3229, Test Loss: 0.1907, Test Acc: 95.80%\n"
     ]
    }
   ],
   "source": [
    "# do training\n",
    "model = MLP(input_size=X_train.shape[1], output_size=1,\n",
    "                hidden_layers=8,\n",
    "                hidden_size=64)\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), 0.001)\n",
    "criterion = nn.BCELoss()\n",
    "acc = train_and_eval(model, train_loader, test_loader, optimizer, criterion, epochs=20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "24"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "# calculate the fairness matrices on the testing dataset\n",
    "with torch.no_grad():\n",
    "    X = torch.tensor(test_features.values, dtype=torch.float32).to(device)\n",
    "    Y = torch.tensor(test_labels.values.reshape(-1, 1), dtype=torch.float32).to(device)\n",
    "    predicted_Y = model(X).cpu().numpy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "data": {
      "text/plain": "(9239, 1)"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_Y.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall:  0.4669603524229075\n",
      "precision:  0.2872628726287263\n",
      "f1:  0.3557046979865772\n",
      "auc:  0.7188885206411031\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import *\n",
    "pred = predicted_Y>0.5\n",
    "recall = recall_score(test_labels, pred)\n",
    "precision = precision_score(test_labels, pred)\n",
    "f1 = f1_score(test_labels, pred)\n",
    "auc = roc_auc_score(test_labels, pred)\n",
    "print('recall: ', recall)\n",
    "print('precision: ', precision)\n",
    "print('f1: ', f1)\n",
    "print('auc: ', auc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the roc curve\n",
    "import matplotlib.pyplot as plt\n",
    "probas_ = predicted_Y.copy()\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, probas_)\n",
    "auc = roc_auc_score(test_labels, probas_)\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.4f)' % auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve for cpu model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the precision recall curve\n",
    "precision, recall, _ = precision_recall_curve(test_labels, probas_)\n",
    "average_precision = average_precision_score(test_labels, probas_)\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision-Recall curve for cpu model: AP={0:0.2f}'.format(\n",
    "          average_precision))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold=0.222405, G-Mean=0.782\n",
      "tpr:  0.7004405286343612 fpr:  0.12660896582334666\n",
      "precision:  0.12230769230769231\n",
      "recall:  0.7004405286343612\n",
      "f1_score:  0.20825147347740666\n",
      "thresholdOpt:  0.61\n",
      "tpr:  0.3876651982378855 fpr:  0.012982689747003996\n",
      "precision:  0.4292682926829268\n",
      "recall:  0.3876651982378855\n",
      "f1_score:  0.4074074074074074\n",
      "accuracy:  0.9722913735252733\n"
     ]
    }
   ],
   "source": [
    "# find the optimal threshold on roc curve\n",
    "probas_ = predicted_Y.copy()\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, probas_)\n",
    "gmeans = np.sqrt(tpr * (1-fpr))\n",
    "ix = np.argmax(gmeans)\n",
    "thresholdOpt = thresholds[ix]\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (thresholdOpt, gmeans[ix]))\n",
    "print('tpr: ', tpr[ix], 'fpr: ', fpr[ix])\n",
    "print('precision: ', precision_score(test_labels, probas_ >= thresholdOpt))\n",
    "print('recall: ', recall_score(test_labels, probas_ >= thresholdOpt))\n",
    "print('f1_score: ', f1_score(test_labels, probas_ >= thresholdOpt))\n",
    "\n",
    "# find the threshold that maximize the f1 score\n",
    "thresholds_f1 = np.arange(0, 1, 0.01)\n",
    "f1_scores = []\n",
    "fpr_f1 = []\n",
    "tpr_f1 = []\n",
    "accuracy_f1 = []\n",
    "precision_f1 = []\n",
    "for threshold in thresholds_f1:\n",
    "    preds_f1 = probas_ >= threshold\n",
    "    f1_scores.append(f1_score(test_labels, preds_f1))\n",
    "    tn, fp, fn, tp = confusion_matrix(test_labels, preds_f1).ravel()\n",
    "    fpr_f1.append(fp / (fp + tn))\n",
    "    tpr_f1.append(tp / (tp + fn))\n",
    "    accuracy_f1.append((tp + tn) / (tp + tn + fp + fn))\n",
    "    precision_f1.append(tp / (tp + fp))\n",
    "thresholdOpt_f1 = thresholds_f1[np.argmax(f1_scores)]\n",
    "print('thresholdOpt: ', thresholdOpt_f1)\n",
    "print('tpr: ', tpr_f1[np.argmax(f1_scores)], 'fpr: ', fpr_f1[np.argmax(f1_scores)])\n",
    "print('precision: ', precision_f1[np.argmax(f1_scores)])\n",
    "print('recall: ', recall_score(test_labels, probas_ >= thresholdOpt_f1))\n",
    "print('f1_score: ', np.max(f1_scores))\n",
    "print('accuracy: ', accuracy_f1[np.argmax(f1_scores)])\n",
    "# precision = tp / (tp + fp)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "# save the MLP model\n",
    "import pickle\n",
    "save_dir = \"./models/model_mlp.pth\"\n",
    "torch.save(model.state_dict(), save_dir)\n",
    "with open('./models/precision_mlp.pkl', 'wb') as f:\n",
    "    pickle.dump(precision_f1, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
